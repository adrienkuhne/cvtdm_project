geom_vline(aes(xintercept = mean(df$ph,na.rm = T), color ="mean")) +
scale_color_manual(values = c("red","blue"))  +
theme_ipsum() +
labs(fill="Quality of the water", title= "Distribution of ph for good and bad water", color = "Mean and median")
ggplot(df, aes(x=ph, fill=factor(Potability))) +
geom_histogram( color="#e9ecef", alpha=0.6, position = 'identity') +
scale_fill_manual(values=c("#69b3a2", "#404080"), labels = c("Non-potable","Potable")) +
geom_vline(aes(xintercept = median(df$ph, na.rm=T), color="median")) +
geom_vline(aes(xintercept = mean(df$ph,na.rm = T), color ="mean")) +
scale_color_manual(values = c("red","blue"))  +
theme_ipsum() +
labs(fill="Quality of the water", title= "Distribution of ph for good and bad water", color = "Mean and median")
potable_amount = sum(df$Potability == 1)/nrow(df)
notpotable_amount = sum(df$Potability == 0) / nrow(df)
pot_perc = paste0(round(potable_amount,4)*100, "% : Potable")
notpot_perc = paste0(round(notpotable_amount,4)*100, "% : Non-Potable")
amount = data.frame("Group" =c("Potable","Non-Potable"),"Percentage"=c(potable_amount, notpotable_amount), "Labels" = c(pot_perc,notpot_perc))
pie_chart = ggplot(amount, aes(x = "", y = Percentage, fill = Group)) +
geom_col(color = "black") +
geom_label(aes(label = Labels), color = c(1, "white"),
position = position_stack(vjust = 0.5),
show.legend = FALSE) +
guides(fill = guide_legend(title = "Quality of the Water"))+
scale_fill_viridis_d() +
coord_polar(theta = "y") + labs(title = "Proportion of Potable and Non-Potable Water") +
theme_void()
amount$total = c(sum(df$Potability == 1),sum(df$Potability == 0))
bar_chart = ggplot(amount, aes(x = Group, y = total, fill = Group)) + geom_bar(stat = "identity") +
scale_fill_viridis_d() + labs(title = "Quantity of good and bad water in our data set", x = "Water Quality", y = "Total amount", fill = "Water Quality")
plot_grid(pie_chart, bar_chart, labels = "AUTO")
set.seed(1)
tr_control = trainControl(method = "cv",number = 5 ,summaryFunction =  multiClassSummary) # Setting 5 fold cross-validation
normal_cv_tree = train(Potability ~ ., #. is to tell that we use all our variables as predictors on personal loan
data = train.set,    ## on our training data
method = "rpart",    ### we use classification tree
na.action = na.pass,
preProcess = c("center","scale"),
trControl = tr_control)   ### the cross validation on 10 folds  ### on 30 k
#,metric = "Kappa"# We can play with metrics to select one specific but here I want them all
print(normal_cv_tree)
accu1 = max(normal_cv_tree$results$Accuracy)
f11 = max(normal_cv_tree$results$F1)
#results = data.frame("Accuracy" = accu1, )
set.seed(1)
tr_control = trainControl(method = "cv",number = 5 ,summaryFunction =  multiClassSummary) # Setting 5 fold cross-validation
normal_cv_tree = train(Potability ~ ., #. is to tell that we use all our variables as predictors on personal loan
data = train.set,    ## on our training data
method = "rpart",    ### we use classification tree
na.action = na.pass,
preProcess = c("center","scale"),
trControl = tr_control)   ### the cross validation on 10 folds  ### on 30 k
#,metric = "Kappa"# We can play with metrics to select one specific but here I want them all
print(normal_cv_tree)
accu1 = max(normal_cv_tree$results$Accuracy)
f11 = max(normal_cv_tree$results$F1)
#results = data.frame("Accuracy" = accu1, )
results_cv_impute = data.frame("Accuracy" = c(accu1, med_acc, knn_acc, drop_acc), "F1 Score" = c(f11, med_f1, knn_f1, drop_f1),"Method" = c("No dropping","Median imputation","K-NN imputation","Dropping"))
results_cv_impute %>% gather(Attributes, Values, 1:2) %>% ggplot(aes(x = Attributes, y = Values, fill = Method)) + scale_fill_viridis_d() + geom_bar(stat = "identity",position = "dodge") + geom_text(mapping= aes(x = Attributes, y=Values,label = round(Values,4)), vjust=-0.5, size = 2,position = position_dodge(width = .9), col = "red") + labs(title = "Comparison of the methods", x = "Metrics") + theme_light()
results_cv_impute = data.frame("Accuracy" = c(accu1, med_acc, knn_acc, drop_acc), "F1 Score" = c(f11, med_f1, knn_f1, drop_f1),"Method" = c("No dropping","Median imputation","K-NN imputation","Dropping the NAs"))
results_cv_impute %>% gather(Attributes, Values, 1:2) %>% ggplot(aes(x = Attributes, y = Values, fill = Method)) + scale_fill_viridis_d() + geom_bar(stat = "identity",position = "dodge") + geom_text(mapping= aes(x = Attributes, y=Values,label = round(Values,4)), vjust=-0.5, size = 2,position = position_dodge(width = .9), col = "red") + labs(title = "Comparison of the methods", x = "Metrics") + theme_light()
results_cv_impute = data.frame("Accuracy" = c(accu1, med_acc, knn_acc, drop_acc), "F1 Score" = c(f11, med_f1, knn_f1, drop_f1),"Method" = c("No dropping","Median imputation","K-NN imputation","Dropping the NAs"))
results_cv_impute %>% gather(Attributes, Values, 1:2) %>% ggplot(aes(x = Attributes, y = Values, fill = Method)) + scale_fill_viridis_d() + geom_bar(stat = "identity",position = "dodge") + geom_text(mapping= aes(x = Attributes, y=Values,label = round(Values,4)), vjust=-0.5, size = 2,position = position_dodge(width = .9), col = "red") + labs(title = "Comparison of the methods to dealwith the NAs", x = "Metrics") + theme_light()
rm(list = ls())
df = read.csv("water_potability.csv")
library(naniar) #### missing values graph
library(ggplot2) ### plotting library
library(tidyverse)
library(plotly) #### nice interactive graph
library(rpart)
library(rpart.plot)
library(forecast) ### forcast accuracy
library(caret) ### confusion matrix, CV, preprocess and more
library(cowplot) ## multiple ggplot subplot
library(hrbrthemes) ### nice ggplot themes
library(mice) ### missing valiues
library(pheatmap) ### Nice heatmaps
library(fmsb) ### Radar chart
str(df)
summary(df)
potable_amount = sum(df$Potability == 1)/nrow(df)
notpotable_amount = sum(df$Potability == 0) / nrow(df)
pot_perc = paste0(round(potable_amount,4)*100, "% : Potable")
notpot_perc = paste0(round(notpotable_amount,4)*100, "% : Non-Potable")
amount = data.frame("Group" =c("Potable","Non-Potable"),"Percentage"=c(potable_amount, notpotable_amount), "Labels" = c(pot_perc,notpot_perc))
pie_chart = ggplot(amount, aes(x = "", y = Percentage, fill = Group)) +
geom_col(color = "black") +
geom_label(aes(label = Labels), color = c(1, "white"),
position = position_stack(vjust = 0.5),
show.legend = FALSE) +
guides(fill = guide_legend(title = "Quality of the Water"))+
scale_fill_viridis_d() +
coord_polar(theta = "y") + labs(title = "Proportion of Potable and Non-Potable Water") +
theme_void()
amount$total = c(sum(df$Potability == 1),sum(df$Potability == 0))
bar_chart = ggplot(amount, aes(x = Group, y = total, fill = Group)) + geom_bar(stat = "identity") +
scale_fill_viridis_d() + labs(title = "Quantity of good and bad water in our data set", x = "Water Quality", y = "Total amount", fill = "Water Quality")
plot_grid(pie_chart, bar_chart, labels = "AUTO")
par(mfrow=c(3,3))
for (i in seq(1:9)) {
avg_0 = mean(df[df$Potability == 0, i],na.rm = TRUE)
avg_1 = mean(df[df$Potability == 1, i],na.rm = TRUE)
means = c(avg_0, avg_1)
boxplot(df[,i]~df$Potability, main = paste(colnames(df[i])), xlab = "Water Quality", ylab = colnames(df[i]), col = c("#69b3a2", "#404080"))
points(1:2,means, col = "red", pch = 19)
legend("topright",legend="Means", col = "red", pch =19)
}
ggplot(df, aes(x=ph, fill=factor(Potability))) +
geom_histogram( color="#e9ecef", alpha=0.6, position = 'identity') +
scale_fill_manual(values=c("#69b3a2", "#404080"), labels = c("Non-potable","Potable")) +
geom_vline(aes(xintercept = median(df$ph, na.rm=T), color="median")) +
geom_vline(aes(xintercept = mean(df$ph,na.rm = T), color ="mean")) +
scale_color_manual(values = c("red","blue"))  +
theme_ipsum() +
labs(fill="Quality of the water", title= "Distribution of ph for good and bad water", color = "Mean and median")
ggplot(df, aes(x=Hardness, fill=factor(Potability))) +
geom_histogram( color="#e9ecef", alpha=0.6, position = 'identity') +
scale_fill_manual(values=c("#69b3a2", "#404080"), labels = c("Non-potable","Potable")) +
geom_vline(aes(xintercept = median(df$Hardness, na.rm=T), color="median")) +
geom_vline(aes(xintercept = mean(df$Hardness,na.rm = T), color ="mean")) +
scale_color_manual(values = c("red","blue"))  +
theme_ipsum() +
labs(fill="Quality of the water", title= "Distribution of Hardness for good and bad water", color = "Mean and median")
#median(df$Hardness, na.rm=T)
#mean(df$Hardness,na.rm = T)
ggplot(df, aes(x=Solids, fill=factor(Potability))) +
geom_histogram( color="#e9ecef", alpha=0.6, position = 'identity') +
scale_fill_manual(values=c("#69b3a2", "#404080"), labels = c("Non-potable","Potable")) +
geom_vline(aes(xintercept = median(df$Solids, na.rm=T), color="median")) +
geom_vline(aes(xintercept = mean(df$Solids,na.rm = T), color ="mean")) +
scale_color_manual(values = c("red","blue"))  +
theme_ipsum() +
labs(fill="Quality of the water", title= "Distribution of Solids for good and bad water", color = "Mean and median")
ggplot(df, aes(x=Chloramines, fill=factor(Potability))) +
geom_histogram( color="#e9ecef", alpha=0.6, position = 'identity') +
scale_fill_manual(values=c("#69b3a2", "#404080"), labels = c("Non-potable","Potable")) +
geom_vline(aes(xintercept = median(df$Chloramines, na.rm=T), color="median")) +
geom_vline(aes(xintercept = mean(df$Chloramines,na.rm = T), color ="mean")) +
scale_color_manual(values = c("red","blue"))  +
theme_ipsum() +
labs(fill="Quality of the water", title= "Distribution of Chloramines for good and bad water", color = "Mean and median")
ggplot(df, aes(x=Sulfate, fill=factor(Potability))) +
geom_histogram( color="#e9ecef", alpha=0.6, position = 'identity') +
scale_fill_manual(values=c("#69b3a2", "#404080"), labels = c("Non-potable","Potable")) +
geom_vline(aes(xintercept = median(df$Sulfate, na.rm=T), color="median")) +
geom_vline(aes(xintercept = mean(df$Sulfate,na.rm = T), color ="mean")) +
scale_color_manual(values = c("red","blue"))  +
theme_ipsum() +
labs(fill="Quality of the water", title= "Distribution of Sulfate for good and bad water", color = "Mean and median")
ggplot(df, aes(x=Conductivity, fill=factor(Potability))) +
geom_histogram( color="#e9ecef", alpha=0.6, position = 'identity') +
scale_fill_manual(values=c("#69b3a2", "#404080"), labels = c("Non-potable","Potable")) +
geom_vline(aes(xintercept = median(df$Conductivity, na.rm=T), color="median")) +
geom_vline(aes(xintercept = mean(df$Conductivity,na.rm = T), color ="mean")) +
scale_color_manual(values = c("red","blue"))  +
theme_ipsum() +
labs(fill="Quality of the water", title= "Distribution of Conductivity for good and bad water", color = "Mean and median")
ggplot(df, aes(x=Organic_carbon, fill=factor(Potability))) +
geom_histogram( color="#e9ecef", alpha=0.6, position = 'identity') +
scale_fill_manual(values=c("#69b3a2", "#404080"), labels = c("Non-potable","Potable")) +
geom_vline(aes(xintercept = median(df$Organic_carbon, na.rm=T), color="median")) +
geom_vline(aes(xintercept = mean(df$Organic_carbon,na.rm = T), color ="mean")) +
scale_color_manual(values = c("red","blue"))  +
theme_ipsum() +
labs(fill="Quality of the water", title= "Distribution of Organic carbon for good and bad water", color = "Mean and median")
ggplot(df, aes(x=Trihalomethanes, fill=factor(Potability))) +
geom_histogram( color="#e9ecef", alpha=0.6, position = 'identity') +
scale_fill_manual(values=c("#69b3a2", "#404080"), labels = c("Non-potable","Potable")) +
geom_vline(aes(xintercept = median(df$Trihalomethanes, na.rm=T), color="median")) +
geom_vline(aes(xintercept = mean(df$Trihalomethanes,na.rm = T), color ="mean")) +
scale_color_manual(values = c("red","blue"))  +
theme_ipsum() +
labs(fill="Quality of the water", title= "Distribution of Trihalomethanes for good and bad water", color = "Mean and median")
ggplot(df, aes(x=Turbidity, fill=factor(Potability))) +
geom_histogram( color="#e9ecef", alpha=0.6, position = 'identity') +
scale_fill_manual(values=c("#69b3a2", "#404080"), labels = c("Non-potable","Potable")) +
geom_vline(aes(xintercept = median(df$Turbidity, na.rm=T), color="median")) +
geom_vline(aes(xintercept = mean(df$Turbidity,na.rm = T), color ="mean")) +
scale_color_manual(values = c("red","blue"))  +
theme_ipsum() +
labs(fill="Quality of the water", title= "Distribution of Turbidity for good and bad water", color = "Mean and median")
par(mfrow=c(1,2))
df_1 = df[df$Potability == 1,]
df_0 = df[df$Potability == 0,]
#### to to put max min and the value
rad_1 = data.frame(ph = c(max(df_1$ph, na.rm = T),0,mean(df_1$ph, na.rm=T)),
Hardness = c(max(df_1$Hardness),0,mean(df_1$Hardness, na.rm=T)),
Solids = c(max(df_1$Solids),0,mean(df_1$Solids, na.rm=T)),
Chloramines = c(max(df_1$Chloramines),0,mean(df_1$Chloramines, na.rm=T)),
Sulfate = c(max(df_1$Sulfate, na.rm = T),0,mean(df_1$Sulfate, na.rm=T)),
Conductivity = c(max(df_1$Conductivity),0,mean(df_1$Conductivity, na.rm=T)),
Organic_Carbon = c(max(df_1$Organic_carbon),0,mean(df_1$Organic_carbon, na.rm=T)),
Trihalomethanes = c(max(df_1$Trihalomethanes,na.rm=T),0,mean(df_1$Trihalomethanes, na.rm=T)),
Turbidity = c(max(df_1$Turbidity),0,mean(df_1$Turbidity, na.rm=T))
)
rad_2 = data.frame(ph = c(max(df_0$ph, na.rm = T),0,mean(df_0$ph, na.rm=T)),
Hardness = c(max(df_0$Hardness),0,mean(df_0$Hardness, na.rm=T)),
Solids = c(max(df_0$Solids),0,mean(df_0$Solids, na.rm=T)),
Chloramines = c(max(df_0$Chloramines),0,mean(df_0$Chloramines, na.rm=T)),
Sulfate = c(max(df_0$Sulfate, na.rm = T),0,mean(df_0$Sulfate, na.rm=T)),
Conductivity = c(max(df_0$Conductivity),0,mean(df_0$Conductivity, na.rm=T)),
Organic_Carbon = c(max(df_0$Organic_carbon),0,mean(df_0$Organic_carbon, na.rm=T)),
Trihalomethanes = c(max(df_0$Trihalomethanes,na.rm=T),0,mean(df_0$Trihalomethanes, na.rm=T)),
Turbidity = c(max(df_0$Turbidity),0,mean(df_0$Turbidity, na.rm=T))
)
# Color vector
colors_border=c( rgb(0.2,0.5,0.5,0.9), rgb(0.8,0.2,0.5,0.9) , rgb(0.7,0.5,0.1,0.9) )
colors_in=c( rgb(0.2,0.5,0.5,0.4), rgb(0.8,0.2,0.5,0.4) , rgb(0.7,0.5,0.1,0.4) )
radarchart(rad_1  , axistype=1 ,
#custom polygon
pcol=colors_border , pfcol=colors_in , plwd=4 , plty=1,
#custom the grid
cglcol="grey", cglty=1, axislabcol="grey", cglwd=0.8,
#custom labels
vlcex=0.8 , title = "Potable Water"
)
radarchart(rad_2  , axistype=1 ,
#custom polygon
pcol=colors_border , pfcol=colors_in , plwd=4 , plty=1,
#custom the grid
cglcol="grey", cglty=1, axislabcol="grey", cglwd=0.8,
#custom labels
vlcex=0.8 , title = "Not Potable Water"
)
new_rad = data.frame(ph = c(max(df$ph, na.rm = T),0,mean(df_0$ph, na.rm=T),mean(df_1$ph, na.rm=T)),
Hardness = c(max(df_0$Hardness),0,mean(df_0$Hardness, na.rm=T),mean(df_1$Hardness, na.rm=T)),
Solids = c(max(df_0$Solids),0,mean(df_0$Solids, na.rm=T),mean(df_1$Solids, na.rm=T)),
Chloramines = c(max(df_0$Chloramines),0,mean(df_0$Chloramines, na.rm=T),mean(df_1$Chloramines, na.rm=T)),
Sulfate = c(max(df_0$Sulfate, na.rm = T),0,mean(df_0$Sulfate, na.rm=T),mean(df_1$Sulfate, na.rm=T)),
Conductivity = c(max(df_0$Conductivity),0,mean(df_0$Conductivity, na.rm=T),mean(df_1$Conductivity, na.rm=T)),
Organic_Carbon = c(max(df_0$Organic_carbon),0,mean(df_0$Organic_carbon, na.rm=T),mean(df_1$Organic_carbon, na.rm=T)),
Trihalomethanes = c(max(df_0$Trihalomethanes,na.rm=T),0,mean(df_0$Trihalomethanes, na.rm=T),mean(df_1$Trihalomethanes,na.rm=T)),
Turbidity = c(max(df_0$Turbidity),0,mean(df_0$Turbidity, na.rm=T),mean(df_1$Turbidity, na.rm=T))
)
row.names(new_rad)[3:4] = c("Not Potable", "Potable")
# Color vector
colors_border=c( rgb(0.2,0.5,0.5,0.9), rgb(0.8,0.2,0.5,0.9) )
colors_in=c( rgb(0.2,0.5,0.5,0.4), rgb(0.8,0.2,0.5,0.4))
# plot with default options:
radarchart( new_rad  , axistype=1 ,
#custom polygon
pcol=colors_border , pfcol=colors_in , plwd=4 , plty=1,
#custom the grid
cglcol="grey", cglty=1, axislabcol="grey", cglwd=0.8,
#custom labels
vlcex=0.8
)
# Add a legend
legend(x=0.7, y=1, legend = rownames(new_rad[-c(1,2),]), bty = "n", pch=20 , col=colors_in , text.col = "grey", cex=1.2, pt.cex=3)
new_rad
pheatmap(cor(df, use = "complete"), display_numbers = T,cluster_rows = F, cluster_cols = F, fontsize_number = 15)
gg_miss_var(df, show_pct = TRUE)
library(missRanger)
library(dlookr)
plot_na_intersect(df)
plot_na_pareto(df)
md.pattern(df,rotate.names = TRUE, plot=TRUE)
library(visdat)
vis_miss(df)
set.seed(1)
df$Potability = as.factor(df$Potability) ### changing our Potability variables into a factor since it's a category !
train.index = createDataPartition(df$Potability, p = .6, list = FALSE)
train.set = df[ train.index,] ### 60%
val.set= df[-train.index,]
head(train.set)
set.seed(1)
tr_control = trainControl(method = "cv",number = 5 ,summaryFunction =  multiClassSummary) # Setting 5 fold cross-validation
normal_cv_tree = train(Potability ~ ., #. is to tell that we use all our variables as predictors on personal loan
data = train.set,    ## on our training data
method = "rpart",    ### we use classification tree
na.action = na.pass,
preProcess = c("center","scale"),
trControl = tr_control)   ### the cross validation on 10 folds  ### on 30 k
#,metric = "Kappa"# We can play with metrics to select one specific but here I want them all
print(normal_cv_tree)
accu1 = max(normal_cv_tree$results$Accuracy)
f11 = max(normal_cv_tree$results$F1)
#results = data.frame("Accuracy" = accu1, )
set.seed(1) ### To have the same results  !!!
tr_control = trainControl(method = "cv",number = 5 ,summaryFunction =  multiClassSummary) ### use cross validation 10 times on 10 different fold and have a summary of all metrics and having a preprocess step of imputing the median to see if it works well
median_cv_tree = train(Potability ~ ., #. is to tell that we use all our variables as predictors on personal loan
data = train.set,    ## on our training data
method = "rpart",    ### we use classification tree
na.action = na.pass,
trControl = tr_control,
preProcess = c("medianImpute","center","scale"))   ### the cross validation on 10 folds  ### on 30 k
#,metric = "Kappa"# We can play with metrics to select one specific but here I want them all
print(median_cv_tree)
med_acc = max(median_cv_tree$results$Accuracy)
med_f1 = max(median_cv_tree$results$F1)
set.seed(1)
tr_control = trainControl(method = "cv",number = 5 ,summaryFunction =  multiClassSummary)
knnimpute_cv_tree = train(Potability ~ ., #. is to tell that we use all our variables as predictors on personal loan
data = train.set,    ## on our training data
method = "rpart",    ### we use classification tree
na.action = na.pass,
trControl = tr_control,
preProcess = c("knnImpute","center","scale"))   ### the cross validation on 10 folds  ### on 30 k
#,metric = "Kappa"# We can play with metrics to select one specific but here I want them all
print(knnimpute_cv_tree)
knn_acc = max(knnimpute_cv_tree$results$Accuracy)
knn_f1 = max(knnimpute_cv_tree$results$F1)
set.seed(1) ### To have the same results  !!!
train.set2 = na.omit(train.set)
tr_control = trainControl(method = "cv",number = 5 ,summaryFunction =  multiClassSummary) ### use cross validation 10 times on 10 different fold and have a summary of all metrics and having a preprocess step of imputing the median to see if it works well
drop_cv_tree = train(Potability ~ ., #. is to tell that we use all our variables as predictors on personal loan
data = train.set2,    ## on our training data
method = "rpart",    ### we use classification tree
na.action = na.pass,
preProcess = c("center","scale"),
trControl = tr_control)   ### the cross validation on 10 folds  ### on 30 k
#,metric = "Kappa"# We can play with metrics to select one specific but here I want them all
print(drop_cv_tree)
drop_acc = max(drop_cv_tree$results$Accuracy)
drop_f1 = max(drop_cv_tree$results$F1)
results_cv_impute = data.frame("Accuracy" = c(accu1, med_acc, knn_acc, drop_acc), "F1 Score" = c(f11, med_f1, knn_f1, drop_f1),"Method" = c("No dropping","Median imputation","K-NN imputation","Dropping the NAs"))
results_cv_impute %>% gather(Attributes, Values, 1:2) %>% ggplot(aes(x = Attributes, y = Values, fill = Method)) + scale_fill_viridis_d() + geom_bar(stat = "identity",position = "dodge") + geom_text(mapping= aes(x = Attributes, y=Values,label = round(Values,4)), vjust=-0.5, size = 2,position = position_dodge(width = .9), col = "red") + labs(title = "Comparison of the methods to deal with the NAs", x = "Metrics") + theme_light()
range_values = preProcess(train.set, method = c("knnImpute", "range")) ### range method that we will use for neural nets and knn mostly
train.range = predict(range_values, train.set)
valid.range = predict(range_values, val.set)
norm_values = preProcess(train.set, method = c("knnImpute", "center","scale")) ### z score method (-mean / sd)
train.norm = predict(norm_values, train.set)
valid.norm = predict(norm_values, val.set)
#test = train.set
#norm_values = preProcess(train.set, method = c("range"))
#test_set = predict(norm_values, test)
scale_back_z_score_train <- function(z,var) {
sd_train = sd(train.set[,var], na.rm = T)
mean_train = mean(train.set[,var], na.rm = T)
x = sapply(z, function(x_) x_*sd_train + mean_train)
return(x)
}
scale_back_z_score_valid <- function(z,var) {
sd_train = sd(val.set[,var], na.rm = T)
mean_train = mean(val.set[,var], na.rm = T)
x = sapply(z, function(x_) x_*sd_train + mean_train)
return(x)
}
knn_set_scale_back = train.norm
#colnames(train.norm[-10]) Could do this to scale back our data
#for (i in colnames(knn_set_scale_back[-10])) {
# knn_set_scale_back[i] = scale_back_z_score(knn_set_scale_back[i],i)
#}
library(reshape2)
ph_group = cbind(train.set$ph, knn_set_scale_back$ph)
ph_group = as.data.frame(ph_group)
colnames(ph_group) = c("ph no imputation", "ph knn imputation")
ph_group = melt(ph_group)
diff_ph = ggplot(ph_group, aes(x=value, group = variable, fill = variable)) + geom_density(alpha = 0.5)+labs(title = "Distribution of PH with and without knn imputation") + scale_fill_manual(values = c("red","blue"))
sulfate_group = cbind(train.set$Sulfate, knn_set_scale_back$Sulfate)
sulfate_group = as.data.frame(sulfate_group)
colnames(sulfate_group) = c("sulfate no imputation", "sulfate knn imputation")
sulfate_group = melt(sulfate_group)
diff_sulfate = ggplot(sulfate_group, aes(x=value, group = variable, fill = variable)) + geom_density(alpha = 0.5)+labs(title = "Distribution of sulfate with and without knn imputation") + scale_fill_manual(values = c("red","blue"))
trihalomethanes_group = cbind(train.set$Trihalomethanes, knn_set_scale_back$Trihalomethanes)
trihalomethanes_group = as.data.frame(trihalomethanes_group)
colnames(trihalomethanes_group) = c("trihalomethanes no imputation", "trihalomethanes knn imputation")
trihalomethanes_group = melt(trihalomethanes_group)
diff_triha = ggplot(trihalomethanes_group, aes(x=value, group = variable, fill = variable)) + geom_density(alpha = 0.5)+labs(title = "Distribution of trihalomethanes with and without knn imputation") + scale_fill_manual(values = c("red","blue"))
plot_grid(diff_ph, diff_sulfate, diff_triha)
fig <- plot_ly(knn_set_scale_back, x = ~ph, y = ~Turbidity, z = ~Organic_carbon, color = ~Potability)
fig <- fig %>% add_markers()
fig <- fig %>% layout(scene = list(xaxis = list(title = 'ph'),
yaxis = list(title = 'Turbidity'),
zaxis = list(title = 'Organic_carbon')))
fig
set.seed(1)
tr_control = trainControl(method = "cv",number = 5 ,summaryFunction =  multiClassSummary)
knn_cross_val = train(Potability ~ ., #. is to tell that we use all our variables as predictors on personal loan
data = train.range,    ## on our training data
method = "knn",    ### we use classification tree
trControl = tr_control,
tuneGrid = data.frame(k = seq(50)))
print(knn_cross_val)
knn_cross_val$results[max(knn_cross_val$results$F1) == knn_cross_val$results$F1, c("k","F1")]
library(class)
knn_res = knn(train.range[-10], valid.range[-10], cl=train.range$Potability,k=42)
confusionMatrix(knn_res, valid.range$Potability,positive = "1")
train_scaled_back = train.norm
valid_scaled_back = valid.norm
#colnames(train.norm[-10]) Could do this to scale back our data
for (i in colnames(train_scaled_back[-10])) {
train_scaled_back[i] = scale_back_z_score(train_scaled_back[i],i)
}
train_scaled_back = train.norm
valid_scaled_back = valid.norm
#colnames(train.norm[-10]) Could do this to scale back our data
for (i in colnames(train_scaled_back[-10])) {
train_scaled_back[i] = scale_back_z_score(train_scaled_back[i],i)
}
train_scaled_back = train.norm
valid_scaled_back = valid.norm
#colnames(train.norm[-10]) Could do this to scale back our data
for (i in colnames(train_scaled_back[-10])) {
train_scaled_back[i] = scale_back_z_score_train(train_scaled_back[i],i)
}
for (i in colnames(valid_scaled_back[-10])) {
valid_scaled_back[i] = scale_back_z_score_valid(valid_scaled_back[i],i)
}
View(valid_scaled_back)
View(val.set)
scale_back_z_score_train <- function(z,var) {
sd_train = sd(train.set[,var], na.rm = T)
mean_train = mean(train.set[,var], na.rm = T)
x = sapply(z, function(x_) x_*sd_train + mean_train)
return(x)
}
scale_back_z_score_valid <- function(z,var) {
sd_train = sd(val.set[,var], na.rm = T)
mean_train = mean(val.set[,var], na.rm = T)
x = sapply(z, function(x_) x_*sd_train + mean_train)
return(x)
}
knn_set_scale_back = train.norm
#colnames(train.norm[-10]) Could do this to scale back our data
#for (i in colnames(knn_set_scale_back[-10])) {
# knn_set_scale_back[i] = scale_back_z_score(knn_set_scale_back[i],i)
#}
train_scaled_back = train.norm
valid_scaled_back = valid.norm
#colnames(train.norm[-10]) Could do this to scale back our data
for (i in colnames(train_scaled_back[-10])) {
train_scaled_back[i] = scale_back_z_score_train(train_scaled_back[i],i)
}
for (i in colnames(valid_scaled_back[-10])) {
valid_scaled_back[i] = scale_back_z_score_valid(valid_scaled_back[i],i)
}
View(valid_scaled_back)
View(val.set)
scale_back_z_score_train <- function(z,var) {
sd_train = sd(train.set[,var], na.rm = T)
mean_train = mean(train.set[,var], na.rm = T)
x = sapply(z, function(x_) x_*sd_train + mean_train)
return(x)
}
scale_back_z_score_valid <- function(z,var) {
sd_val = sd(val.set[,var], na.rm = T)
mean_val = mean(val.set[,var], na.rm = T)
x = sapply(z, function(x_) x_*sd_val + mean_val)
return(x)
}
knn_set_scale_back = train.norm
#colnames(train.norm[-10]) Could do this to scale back our data
#for (i in colnames(knn_set_scale_back[-10])) {
# knn_set_scale_back[i] = scale_back_z_score(knn_set_scale_back[i],i)
#}
train_scaled_back = train.norm
valid_scaled_back = valid.norm
#colnames(train.norm[-10]) Could do this to scale back our data
for (i in colnames(train_scaled_back[-10])) {
train_scaled_back[i] = scale_back_z_score_train(train_scaled_back[i],i)
}
for (i in colnames(valid_scaled_back[-10])) {
valid_scaled_back[i] = scale_back_z_score_valid(valid_scaled_back[i],i)
}
View(val.set)
View(valid_scaled_back)
set.seed(1)
df$Potability = as.factor(df$Potability) ### changing our Potability variables into a factor since it's a category !
train.index = createDataPartition(df$Potability, p = .6, list = FALSE)
train.set = df[ train.index,] ### 60%
val.set= df[-train.index,]
head(train.set)
scale_back_z_score <- function(z,var) {
sd_train = sd(train.set[,var], na.rm = T)
mean_train = mean(train.set[,var], na.rm = T)
x = sapply(z, function(x_) x_*sd_train + mean_train)
return(x)
}
scale_back_z_score_valid <- function(z,var) {
sd_val = sd(val.set[,var], na.rm = T)
mean_val = mean(val.set[,var], na.rm = T)
x = sapply(z, function(x_) x_*sd_val + mean_val)
return(x)
}
knn_set_scale_back = train.norm
#colnames(train.norm[-10]) Could do this to scale back our data
#for (i in colnames(knn_set_scale_back[-10])) {
# knn_set_scale_back[i] = scale_back_z_score(knn_set_scale_back[i],i)
#}
train_scaled_back = train.norm
valid_scaled_back = valid.norm
#colnames(train.norm[-10]) Could do this to scale back our data
for (i in colnames(train_scaled_back[-10])) {
train_scaled_back[i] = scale_back_z_score(train_scaled_back[i],i)
}
for (i in colnames(valid_scaled_back[-10])) {
valid_scaled_back[i] = scale_back_z_score(valid_scaled_back[i],i)
}
View(valid_scaled_back)
View(val.set)
train_scaled_back = train.norm
valid_scaled_back = valid.norm
#colnames(train.norm[-10]) Could do this to scale back our data
for (i in colnames(train_scaled_back[-10])) {
train_scaled_back[i] = scale_back_z_score(train_scaled_back[i],i)
}
for (i in colnames(valid_scaled_back[-10])) {
valid_scaled_back[i] = scale_back_z_score(valid_scaled_back[i],i)
}
View(valid_scaled_back)
View(train.set)
View(train_scaled_back)
ggplot(df, aes(x=Chloramines, fill=factor(Potability))) +
geom_histogram( color="#e9ecef", alpha=0.6, position = 'identity') +
scale_fill_manual(values=c("#69b3a2", "#404080"), labels = c("Non-potable","Potable")) +
geom_vline(aes(xintercept = median(df$Chloramines, na.rm=T), color="median")) +
geom_vline(aes(xintercept = mean(df$Chloramines,na.rm = T), color ="mean")) +
geom_vline(aes(xintercept = 0, color ="WHO recommendation")) +
geom_vline(aes(xintercept = 4, color ="WHO recommendation")) +
scale_color_manual(values = c("red","blue","black"))  +
theme_ipsum() +
labs(fill="Quality of the water", title= "Distribution of Chloramines for potable and non-potable water", color = "Mean and median")
