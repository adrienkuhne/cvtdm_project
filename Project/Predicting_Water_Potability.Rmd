---
title: "Predicting Water Potability"
author: "Romain Kursner & Adrien Kühne"
date: "11/18/2021"
output: html_document
---

### Loading the data set : 
```{r}
rm(list = ls())
df = read.csv("water_potability.csv")
```

### Loading our libraries :
```{r}
library(naniar) #### missing values graph
library(ggplot2) ### plotting library
library(tidyverse)
library(plotly) #### nice interactive graph
library(rpart)
library(rpart.plot)
library(forecast) ### forcast accuracy
library(caret) ### confusion matrix, CV, preprocess and more
library(cowplot) ## multiple ggplot subplot
library(hrbrthemes) ### nice ggplot themes
library(mice) ### missing valiues 
library(pheatmap) ### Nice heatmaps
library(fmsb) ### Radar chart
```

### Exploratory data analysis :

```{r}
str(df)
```

As you can see, our data set has 3276 observations and 10 variables. They are all numerical values except 'Potability' which is an integer. It is normal since 'Potability' indicates if our water is either potable or not. We will need to transform it into a factor later on. We can already see some missing values.

```{r}
summary(df)
```

Here we can see the minimum, maximum, mean and median of all our variables. What is really important to notice is that we have 491 missing values in 'ph', 781 in 'Sulfate' and 162 in 'Trihalomethanes.' We will need to find a good strategy to deal with them.

We could check if we have a balanced data since we are going to classify our water based on the 'Potability' variable.

```{r}
potable_amount = sum(df$Potability == 1)/nrow(df)
notpotable_amount = sum(df$Potability == 0) / nrow(df)
pot_perc = paste0(round(potable_amount,4)*100, "% : Potable")
notpot_perc = paste0(round(notpotable_amount,4)*100, "% : Non-Potable")

amount = data.frame("Group" =c("Potable","Non-Potable"),"Percentage"=c(potable_amount, notpotable_amount), "Labels" = c(pot_perc,notpot_perc))

pie_chart = ggplot(amount, aes(x = "", y = Percentage, fill = Group)) +
  geom_col(color = "black") +
  geom_label(aes(label = Labels), color = c(1, "white"),
            position = position_stack(vjust = 0.5),
            show.legend = FALSE) +
  guides(fill = guide_legend(title = "Quality of the Water"))+
  scale_fill_viridis_d() +
  coord_polar(theta = "y") + labs(title = "Proportion of Potable and Non-Potable Water") +
  theme_void()

amount$total = c(sum(df$Potability == 1),sum(df$Potability == 0))

bar_chart = ggplot(amount, aes(x = Group, y = total, fill = Group)) + geom_bar(stat = "identity") +
  scale_fill_viridis_d() + labs(title = "Quantity of good and bad water in our data set", x = "Water Quality", y = "Total amount", fill = "Water Quality")

plot_grid(pie_chart, bar_chart, labels = "AUTO")

```

We can see that we have more non-potable than potable water. In general, even if we have more non-potable water, our data set is pretty balanced. We have 1278 Potable and 1998 non-potable water. I think we can focus our analysis on accuracy and F1 score to be more robust.

### Distribution of all our variables by using some boxplots : 

```{r, fig.height=9, fig.width=9}
par(mfrow=c(3,3))
for (i in seq(1:9)) {
  avg_0 = mean(df[df$Potability == 0, i],na.rm = TRUE)
  avg_1 = mean(df[df$Potability == 1, i],na.rm = TRUE)
  means = c(avg_0, avg_1)
  boxplot(df[,i]~df$Potability, main = paste(colnames(df[i])), xlab = "Water Quality", ylab = colnames(df[i]), col = c("#69b3a2", "#404080"))
  points(1:2,means, col = "red", pch = 19)
  legend("topright",legend="Means", col = "red", pch =19)
  
}
```

The distribution of both groups of water seems pretty similar. But we can clearly see something weird going on. First I would think that there would be a difference between the 2 groups. Second, the who recommends that the desirable limit of 'Solids' is 500 mg/liter and maximum limit of 'Solids' is 1000mg/liter (WHO) but we can clearly see that we have potable water that are well above this recommendation.

### Distribution of all our variables by using some histograms : 

```{r}
ggplot(df, aes(x=ph, fill=factor(Potability))) +
    geom_histogram( color="#e9ecef", alpha=0.6, position = 'identity') +
    scale_fill_manual(values=c("#69b3a2", "#404080"), labels = c("Non-potable","Potable")) +
    geom_vline(aes(xintercept = median(df$ph, na.rm=T), color="median")) +
    geom_vline(aes(xintercept = mean(df$ph,na.rm = T), color ="mean")) +
    geom_vline(aes(xintercept =6.5, color="WHO recommendation")) +
    geom_vline(aes(xintercept =8.5, color="WHO recommendation")) +
    scale_color_manual(values = c("red","blue", "black"))  +
    theme_ipsum() +
    labs(fill="Quality of the water", title= "Distribution of ph for potable and non-potable water", color = "Mean and median")
```

The 'ph' variable seems pretty normally distributed. We can see that the 'ph' variable is close to symmetric (median=mean). The potable and non potable water seem to be normally distributed with pretty much the same value for ph which seems a little bit weird. The WHO recommends to get drink water with a pH from 6.5 to 8.5.  

```{r}
ggplot(df, aes(x=Hardness, fill=factor(Potability))) +
    geom_histogram( color="#e9ecef", alpha=0.6, position = 'identity') +
    scale_fill_manual(values=c("#69b3a2", "#404080"), labels = c("Non-potable","Potable")) +
    geom_vline(aes(xintercept = median(df$Hardness, na.rm=T), color="median")) +
    geom_vline(aes(xintercept = mean(df$Hardness,na.rm = T), color ="mean")) +
    scale_color_manual(values = c("red","blue"))  +
    theme_ipsum() +
    labs(fill="Quality of the water", title= "Distribution of Hardness for potable and non-potable water", color = "Mean and median")
#median(df$Hardness, na.rm=T)
#mean(df$Hardness,na.rm = T)
```

The 'Hardness' variable seems pretty normally distributed. We can see that the 'Hardness' variable is close to symmetric (median=mean).It seems more centered as well. The WHO does not recommend a limit of 'Hardness'. In drinking-water, hardness is in the range 10–500 mg of calcium carbonate per litre source : https://www.who.int/water_sanitation_health/dwq/hardness.pdf


```{r}
ggplot(df, aes(x=Solids, fill=factor(Potability))) +
    geom_histogram( color="#e9ecef", alpha=0.6, position = 'identity') +
    scale_fill_manual(values=c("#69b3a2", "#404080"), labels = c("Non-potable","Potable")) +
    geom_vline(aes(xintercept = median(df$Solids, na.rm=T), color="median")) +
    geom_vline(aes(xintercept = mean(df$Solids,na.rm = T), color ="mean")) +
    geom_vline(aes(xintercept = 500, color ="WHO recommendation")) +
    geom_vline(aes(xintercept = 1000, color ="WHO recommendation")) +
    scale_color_manual(values = c("red","blue", "black"))  +
    theme_ipsum() +
    labs(fill="Quality of the water", title= "Distribution of Solids for potable and non-potable water", color = "Mean and median")

```

The 'Solids' variable seems pretty normally distributed. We can see that the 'Solids' variable is not close to symmetric and is right-skewed (median<mean). The desirable limit of 'Solids' is 500 mg/liter and maximum limit of 'Solids' is 1000mg/liter (WHO). Once again, it seems that our values are not realistic for potable water.

```{r}
ggplot(df, aes(x=Chloramines, fill=factor(Potability))) +
    geom_histogram( color="#e9ecef", alpha=0.6, position = 'identity') +
    scale_fill_manual(values=c("#69b3a2", "#404080"), labels = c("Non-potable","Potable")) +
    geom_vline(aes(xintercept = median(df$Chloramines, na.rm=T), color="median")) +
    geom_vline(aes(xintercept = mean(df$Chloramines,na.rm = T), color ="mean")) +
    geom_vline(aes(xintercept = 0, color ="WHO recommendation")) +
    geom_vline(aes(xintercept = 4, color ="WHO recommendation")) +
    scale_color_manual(values = c("red","blue","black"))  +
    theme_ipsum() +
    labs(fill="Quality of the water", title= "Distribution of Chloramines for potable and non-potable water", color = "Mean and median")

```

The 'Chloramines' variable seems pretty normally distributed. We can see that the 'Chloramines' variable is close to symmetric (median=mean). It seems more centered as well. The desirable limit of 'Chloramines' is 4 mg/liter (WHO). Once again, it seems that our values are not realistic.

```{r}
ggplot(df, aes(x=Sulfate, fill=factor(Potability))) +
    geom_histogram( color="#e9ecef", alpha=0.6, position = 'identity') +
    scale_fill_manual(values=c("#69b3a2", "#404080"), labels = c("Non-potable","Potable")) +
    geom_vline(aes(xintercept = median(df$Sulfate, na.rm=T), color="median")) +
    geom_vline(aes(xintercept = mean(df$Sulfate,na.rm = T), color ="mean")) +
    scale_color_manual(values = c("red","blue"))  +
    theme_ipsum() +
    labs(fill="Quality of the water", title= "Distribution of Sulfate for potable and non-potable water", color = "Mean and median")

```

The 'Sulfate' variable seems pretty normally distributed. We can see that the 'Sulfate' variable is close to symmetric (median=mean).It seems more centered as well. The WHO recommend a level of 500 mg/l as a limit for sulfate.

```{r}
ggplot(df, aes(x=Conductivity, fill=factor(Potability))) +
    geom_histogram( color="#e9ecef", alpha=0.6, position = 'identity') +
    scale_fill_manual(values=c("#69b3a2", "#404080"), labels = c("Non-potable","Potable")) +
    geom_vline(aes(xintercept = median(df$Conductivity, na.rm=T), color="median")) +
    geom_vline(aes(xintercept = mean(df$Conductivity,na.rm = T), color ="mean")) +
    geom_vline(aes(xintercept = 0, color ="WHO recommendation")) +
    geom_vline(aes(xintercept = 400, color ="WHO recommendation")) +
    scale_color_manual(values = c("red","blue", "black"))  +
    theme_ipsum() +
    labs(fill="Quality of the water", title= "Distribution of Conductivity for potable and non-potable water", color = "Mean and median")

```

The 'Conductivity' variable seems pretty normally distributed. Even though the 'Conductivity' variable is slightly right-skewed (median<mean), we can see that it is close to symmetric (median=mean).The desirable limit of 'Conductivity' should not exceed 400 μS/cm (WHO). 

```{r}
ggplot(df, aes(x=Organic_carbon, fill=factor(Potability))) +
    geom_histogram( color="#e9ecef", alpha=0.6, position = 'identity') +
    scale_fill_manual(values=c("#69b3a2", "#404080"), labels = c("Non-potable","Potable")) +
    geom_vline(aes(xintercept = median(df$Organic_carbon, na.rm=T), color="median")) +
    geom_vline(aes(xintercept = mean(df$Organic_carbon,na.rm = T), color ="mean")) +
  geom_vline(aes(xintercept = 2, color ="Recommendation")) +
    scale_color_manual(values = c("red","blue", "black"))  +
    theme_ipsum() +
    labs(fill="Quality of the water", title= "Distribution of Organic carbon for potable and non-potable water", color = "Mean and median")

```

The 'Organic_carbon' variable seems pretty normally distributed. We can see that the 'Organic_carbon' variable is close to symmetric (median=mean). The recommendation is that water should note have higher than 2mg/L of organic carbon to be drinkable. https://www.canada.ca/en/health-canada/programs/consultation-organic-matter-drinking-water/document.html

```{r}
ggplot(df, aes(x=Trihalomethanes, fill=factor(Potability))) +
    geom_histogram( color="#e9ecef", alpha=0.6, position = 'identity') +
    scale_fill_manual(values=c("#69b3a2", "#404080"), labels = c("Non-potable","Potable")) +
    geom_vline(aes(xintercept = median(df$Trihalomethanes, na.rm=T), color="median")) +
    geom_vline(aes(xintercept = mean(df$Trihalomethanes,na.rm = T), color ="mean")) +
    geom_vline(aes(xintercept = 0, color ="WHO recommendation")) +
    geom_vline(aes(xintercept = 80, color ="WHO recommendation")) +
    scale_color_manual(values = c("red","blue","black"))  +
    theme_ipsum() +
    labs(fill="Quality of the water", title= "Distribution of Trihalomethanes for potable and non-potable water", color = "Mean and median")

```

The 'Trihalomethanes' variable seems pretty normally distributed. We can see that the 'Trihalomethanes' variable is close to symmetric (median=mean). The desirable limit of 'Trihalomethanes' is 80 ppm.

```{r}
ggplot(df, aes(x=Turbidity, fill=factor(Potability))) +
    geom_histogram( color="#e9ecef", alpha=0.6, position = 'identity') +
    scale_fill_manual(values=c("#69b3a2", "#404080"), labels = c("Non-potable","Potable")) +
    geom_vline(aes(xintercept = median(df$Turbidity, na.rm=T), color="median")) +
    geom_vline(aes(xintercept = mean(df$Turbidity,na.rm = T), color ="mean")) +
    geom_vline(aes(xintercept = 0, color ="WHO recommendation")) +
    geom_vline(aes(xintercept = 5, color ="WHO recommendation")) +
    scale_color_manual(values = c("red","blue","black"))  +
    theme_ipsum() +
    labs(fill="Quality of the water", title= "Distribution of Turbidity for potable and non-potable water", color = "Mean and median")

```

The 'Turbidity' variable seems pretty normally distributed. We can see that the 'Turbidity' variable is close to symmetric (median=mean). The desirable limit of 'Turbidity' is 5 NTU (WHO).

### Difference between our potable and non-potable water by using a radar chart.

```{r, fig.width=8, fig.height=8}

par(mfrow=c(1,2))

df_1 = df[df$Potability == 1,]
df_0 = df[df$Potability == 0,]

#### to to put max min and the value

rad_1 = data.frame(ph = c(max(df_1$ph, na.rm = T),0,mean(df_1$ph, na.rm=T)),
                   Hardness = c(max(df_1$Hardness),0,mean(df_1$Hardness, na.rm=T)),
                   Solids = c(max(df_1$Solids),0,mean(df_1$Solids, na.rm=T)),
                   Chloramines = c(max(df_1$Chloramines),0,mean(df_1$Chloramines, na.rm=T)),
                   Sulfate = c(max(df_1$Sulfate, na.rm = T),0,mean(df_1$Sulfate, na.rm=T)),
                   Conductivity = c(max(df_1$Conductivity),0,mean(df_1$Conductivity, na.rm=T)),
                   Organic_Carbon = c(max(df_1$Organic_carbon),0,mean(df_1$Organic_carbon, na.rm=T)),
                   Trihalomethanes = c(max(df_1$Trihalomethanes,na.rm=T),0,mean(df_1$Trihalomethanes, na.rm=T)),
                   Turbidity = c(max(df_1$Turbidity),0,mean(df_1$Turbidity, na.rm=T))
  
                   )

rad_2 = data.frame(ph = c(max(df_0$ph, na.rm = T),0,mean(df_0$ph, na.rm=T)),
                   Hardness = c(max(df_0$Hardness),0,mean(df_0$Hardness, na.rm=T)),
                   Solids = c(max(df_0$Solids),0,mean(df_0$Solids, na.rm=T)),
                   Chloramines = c(max(df_0$Chloramines),0,mean(df_0$Chloramines, na.rm=T)),
                   Sulfate = c(max(df_0$Sulfate, na.rm = T),0,mean(df_0$Sulfate, na.rm=T)),
                   Conductivity = c(max(df_0$Conductivity),0,mean(df_0$Conductivity, na.rm=T)),
                   Organic_Carbon = c(max(df_0$Organic_carbon),0,mean(df_0$Organic_carbon, na.rm=T)),
                   Trihalomethanes = c(max(df_0$Trihalomethanes,na.rm=T),0,mean(df_0$Trihalomethanes, na.rm=T)),
                   Turbidity = c(max(df_0$Turbidity),0,mean(df_0$Turbidity, na.rm=T))

                   )

# Color vector
colors_border=c( rgb(0.2,0.5,0.5,0.9), rgb(0.8,0.2,0.5,0.9) , rgb(0.7,0.5,0.1,0.9) )
colors_in=c( rgb(0.2,0.5,0.5,0.4), rgb(0.8,0.2,0.5,0.4) , rgb(0.7,0.5,0.1,0.4) )

radarchart(rad_1  , axistype=1 ,
 #custom polygon
    pcol=colors_border , pfcol=colors_in , plwd=4 , plty=1,
    #custom the grid
    cglcol="grey", cglty=1, axislabcol="grey", cglwd=0.8,
    #custom labels
    vlcex=0.8 , title = "Potable Water"
    
    )

radarchart(rad_2  , axistype=1 ,
 #custom polygon
    pcol=colors_border , pfcol=colors_in , plwd=4 , plty=1,
    #custom the grid
    cglcol="grey", cglty=1, axislabcol="grey", cglwd=0.8,
    #custom labels
    vlcex=0.8 , title = "Not Potable Water"
    
    )
```

We used the maximum values and 0 when potability = 1 and potability = 0 as the range of the radar chart. We used the mean of each feature for potability = 1 and potability = 0 to represent their characteristics. We can see that there's not a significant difference between them.

```{r}
new_rad = data.frame(ph = c(max(df$ph, na.rm = T),0,mean(df_0$ph, na.rm=T),mean(df_1$ph, na.rm=T)),
                   Hardness = c(max(df_0$Hardness),0,mean(df_0$Hardness, na.rm=T),mean(df_1$Hardness, na.rm=T)),
                   Solids = c(max(df_0$Solids),0,mean(df_0$Solids, na.rm=T),mean(df_1$Solids, na.rm=T)),
                   Chloramines = c(max(df_0$Chloramines),0,mean(df_0$Chloramines, na.rm=T),mean(df_1$Chloramines, na.rm=T)),
                   Sulfate = c(max(df_0$Sulfate, na.rm = T),0,mean(df_0$Sulfate, na.rm=T),mean(df_1$Sulfate, na.rm=T)),
                   Conductivity = c(max(df_0$Conductivity),0,mean(df_0$Conductivity, na.rm=T),mean(df_1$Conductivity, na.rm=T)),
                   Organic_Carbon = c(max(df_0$Organic_carbon),0,mean(df_0$Organic_carbon, na.rm=T),mean(df_1$Organic_carbon, na.rm=T)),
                   Trihalomethanes = c(max(df_0$Trihalomethanes,na.rm=T),0,mean(df_0$Trihalomethanes, na.rm=T),mean(df_1$Trihalomethanes,na.rm=T)),
                   Turbidity = c(max(df_0$Turbidity),0,mean(df_0$Turbidity, na.rm=T),mean(df_1$Turbidity, na.rm=T))

                   )

row.names(new_rad)[3:4] = c("Not Potable", "Potable")


# Color vector
colors_border=c( rgb(0.2,0.5,0.5,0.9), rgb(0.8,0.2,0.5,0.9) )
colors_in=c( rgb(0.2,0.5,0.5,0.4), rgb(0.8,0.2,0.5,0.4))

# plot with default options:
radarchart( new_rad  , axistype=1 , 
    #custom polygon
    pcol=colors_border , pfcol=colors_in , plwd=4 , plty=1,
    #custom the grid
    cglcol="grey", cglty=1, axislabcol="grey", cglwd=0.8,
    #custom labels
    vlcex=0.8 
    )

# Add a legend
legend(x=0.7, y=1, legend = rownames(new_rad[-c(1,2),]), bty = "n", pch=20 , col=colors_in , text.col = "grey", cex=1.2, pt.cex=3)
new_rad

```

In fact, we can see that there is no significant difference between potable and non-potable water. It does not seem realistic at all ! We would expect at least differences between some features.


### Correlation of our variables : 

```{r}
pheatmap(cor(df, use = "complete"), display_numbers = T,cluster_rows = F, cluster_cols = F, fontsize_number = 15)
```

They are not correlated at all between each other. It means that we will not have issues of multicolinearity.

#### Missing values :

```{r}
gg_miss_var(df, show_pct = TRUE)
```

Our initial dataset has 3276 rows and 10 features. It has 1278 observations of potable water and 1998 of non-potable water.
If we drop the missing values, our dataset has 2011 rows and 10 features. It has 811 observations of potable water and 1200 of non-potable water. We would lose 1265 observations. It seems that we have more missing values for the non-potable water.
We can see that almost 25% of the 'Sulfate' and 15% of 'ph' observations have missing values. This is a huge proportion. In comparison, we note that only 5% of the 'Trihalomethanes' observations have missing value. We could check whether the missing values follow a pattern.

#### Potential pattern of the missing values :

```{r}
library(missRanger)
library(dlookr)
plot_na_intersect(df)
```

```{r}
plot_na_pareto(df)
```

```{r}
md.pattern(df,rotate.names = TRUE, plot=TRUE)
```

```{r}
library(visdat)
vis_miss(df)
```

Looking at all the previous graphs, it seems that the missing observations do not really follow a pattern. It does not seem that the NAs are missing in a systematic way.

#### Comparing different methods to find the best one to deal with the missing values. To know which method of imputation performs better, we will need to perform a cross validation using a decision tree as a baseline since it handles missing values. During the Cross-validation, we will impute the missing values based on the median, or knn of the train set on our validation set. For example, with a 5 fold cross validation, it means that 80 % of our training set will be 4 fold and 20% will represent 1 fold. During the Cross validation, the 4 folds will impute its missing values its median or knn than it will do the same of the other fold (the validation one that represents 20%) based on the training value since we want to avoid data leakage. We will choose the method of imputation that performs the best. Here we will use K-NN, median imputation and dropping values and compare it to not dropping values.

Partitioning our data: We will use the stratified sampling method because we want to ensure that we have the same proportion of water quality in each of our samples. We will create a train set that represents 60% our of data and a test set that represents 40 % of our data.

```{r}
set.seed(1)
df$Potability = as.factor(df$Potability) ### changing our Potability variables into a factor since it's a category ! 
train.index = createDataPartition(df$Potability, p = .6, list = FALSE)
train.set = df[ train.index,] ### 60%
test.set= df[-train.index,] ### 40% test
```

```{r}
head(train.set)
```

#### Cross validation with 5 folds for imputation 

Decision tree (without dropping any missing values because DT handle missing values).

```{r}
set.seed(1) 

tr_control = trainControl(method = "cv",number = 5 ,summaryFunction =  multiClassSummary) # Setting 5 fold cross-validation

normal_cv_tree = train(Potability ~ ., #. is to tell that we use all our variables as predictors on personal loan
               data = train.set,    ## on our training data
               method = "rpart",    ### we use classification tree
               na.action = na.pass,
               preProcess = c("center","scale"),
               trControl = tr_control)   ### the cross validation on 10 folds  ### on 30 k
               #,metric = "Kappa"# We can play with metrics to select one specific but here I want them all
               
print(normal_cv_tree)
accu1 = max(normal_cv_tree$results$Accuracy)
f11 = max(normal_cv_tree$results$F1)
#results = data.frame("Accuracy" = accu1, )
```

Without dropping values and by using a decision tree, the accuracy of our model using a 5 fold cross-validation is 0.6327.

Decision tree with median imputation.

```{r}
set.seed(1) ### To have the same results  !!! 

tr_control = trainControl(method = "cv",number = 5 ,summaryFunction =  multiClassSummary) ### use cross validation 10 times on 10 different fold and have a summary of all metrics and having a preprocess step of imputing the median to see if it works well

median_cv_tree = train(Potability ~ ., #. is to tell that we use all our variables as predictors on personal loan
               data = train.set,    ## on our training data
               method = "rpart",    ### we use classification tree
               na.action = na.pass,
               trControl = tr_control,
               preProcess = c("medianImpute","center","scale"))   ### the cross validation on 10 folds  ### on 30 k
               #,metric = "Kappa"# We can play with metrics to select one specific but here I want them all
               
print(median_cv_tree)

med_acc = max(median_cv_tree$results$Accuracy)
med_f1 = max(median_cv_tree$results$F1)
```

We have an accuracy of 0.63428

Decision tree with K-NN imputation. 

```{r}
set.seed(1) 

tr_control = trainControl(method = "cv",number = 5 ,summaryFunction =  multiClassSummary) 

knnimpute_cv_tree = train(Potability ~ ., #. is to tell that we use all our variables as predictors on personal loan
               data = train.set,    ## on our training data
               method = "rpart",    ### we use classification tree
               na.action = na.pass,
               trControl = tr_control,
               preProcess = c("knnImpute","center","scale"))   ### the cross validation on 10 folds  ### on 30 k
               #,metric = "Kappa"# We can play with metrics to select one specific but here I want them all
               
print(knnimpute_cv_tree)

knn_acc = max(knnimpute_cv_tree$results$Accuracy)
knn_f1 = max(knnimpute_cv_tree$results$F1)
```

With the KNN imputation we have an accuracy of 0.63834 using a decision tree.

Decision tree (we drop all the missing values).

```{r}
set.seed(1) ### To have the same results  !!! 

train.set2 = na.omit(train.set)

tr_control = trainControl(method = "cv",number = 5 ,summaryFunction =  multiClassSummary) ### use cross validation 10 times on 10 different fold and have a summary of all metrics and having a preprocess step of imputing the median to see if it works well

drop_cv_tree = train(Potability ~ ., #. is to tell that we use all our variables as predictors on personal loan
               data = train.set2,    ## on our training data
               method = "rpart",    ### we use classification tree
               na.action = na.pass,
               preProcess = c("center","scale"),
               trControl = tr_control)   ### the cross validation on 10 folds  ### on 30 k
               #,metric = "Kappa"# We can play with metrics to select one specific but here I want them all
               
print(drop_cv_tree)

drop_acc = max(drop_cv_tree$results$Accuracy)
drop_f1 = max(drop_cv_tree$results$F1)
```

By dropping our missing values, the decision tree using 5 fold cross-validation gave us an accuracy of 0.6195

#### Comparaison of method of imputation using a 5 fold cross-validation in a decision tree.
```{r}
results_cv_impute = data.frame("Accuracy" = c(accu1, med_acc, knn_acc, drop_acc), "F1 Score" = c(f11, med_f1, knn_f1, drop_f1),"Method" = c("No dropping","Median imputation","K-NN imputation","Dropping the NAs"))

results_cv_impute %>% gather(Attributes, Values, 1:2) %>% ggplot(aes(x = Attributes, y = Values, fill = Method)) + scale_fill_viridis_d() + geom_bar(stat = "identity",position = "dodge") + geom_text(mapping= aes(x = Attributes, y=Values,label = round(Values,4)), vjust=-0.5, size = 2,position = position_dodge(width = .9), col = "red") + labs(title = "Comparison of the methods to deal with the NAs", x = "Metrics") + theme_light()
```

It seems that the K-NN imputation is the best method because it has the highest accuracy and the second F1 score. The median imputation has very similar results (but slightly worse). When we do not drop any NAs, we obtain the highest F1 score but it is very close to the K-NN imputation method and median method. However, for the rest of our project, we need to either impute or drop the missing values to perform, for example, a K-NN model or a neural nets. Therefore, the model with the best results and meeting these criteria is the K-NN imputation.

### Preprocessing using K-NN on our train and valid set. We will do the range and z score method.

we will use 2 different preprocessing methods. The first one using the range method and the second one using the center and scale method. 

```{r}

range_values = preProcess(train.set, method = c("knnImpute", "range")) ### range method that we will use for neural nets and knn mostly

train.range = predict(range_values, train.set)
test.range = predict(range_values, test.set)


norm_values = preProcess(train.set, method = c("knnImpute", "center","scale")) ### z score method (-mean / sd)
train.norm = predict(norm_values, train.set)
test.norm = predict(norm_values, test.set)

#test = train.set
#norm_values = preProcess(train.set, method = c("range"))

#test_set = predict(norm_values, test)

```

### Creating a function to scale back our train and test set from the normalisation of our data. We will need to to work with cross-validation and also to work with tree models and the logistic regression.

```{r}
scale_back_z_score <- function(z,var) {
  sd_train = sd(train.set[,var], na.rm = T)
  mean_train = mean(train.set[,var], na.rm = T)
  x = sapply(z, function(x_) x_*sd_train + mean_train)
  return(x)
}
 

train_scaled_back = train.norm
test_scaled_back = test.norm
#colnames(train.norm[-10]) Could do this to scale back our data
for (i in colnames(train_scaled_back[-10])) {
  train_scaled_back[i] = scale_back_z_score(train_scaled_back[i],i)
}

for (i in colnames(test_scaled_back[-10])) {
  test_scaled_back[i] = scale_back_z_score(test_scaled_back[i],i)
}

```


#### Differences of the distribution when we impute the missing values with and without K-NN:
Here we are going to compare how the K-NN imputation method on the ph, Sulfate and Trihalomethanes variables changed their distribution.
```{r}
library(reshape2)
ph_group = cbind(train.set$ph, train_scaled_back$ph)
ph_group = as.data.frame(ph_group)
colnames(ph_group) = c("ph no imputation", "ph knn imputation")
ph_group = melt(ph_group)
diff_ph = ggplot(ph_group, aes(x=value, group = variable, fill = variable)) + geom_density(alpha = 0.5)+labs(title = "Distribution of PH with and without knn imputation") + scale_fill_manual(values = c("red","blue"))

sulfate_group = cbind(train.set$Sulfate, train_scaled_back$Sulfate)
sulfate_group = as.data.frame(sulfate_group)
colnames(sulfate_group) = c("sulfate no imputation", "sulfate knn imputation")
sulfate_group = melt(sulfate_group)
diff_sulfate = ggplot(sulfate_group, aes(x=value, group = variable, fill = variable)) + geom_density(alpha = 0.5)+labs(title = "Distribution of sulfate with and without knn imputation") + scale_fill_manual(values = c("red","blue"))

trihalomethanes_group = cbind(train.set$Trihalomethanes, train_scaled_back$Trihalomethanes)
trihalomethanes_group = as.data.frame(trihalomethanes_group)
colnames(trihalomethanes_group) = c("trihalomethanes no imputation", "trihalomethanes knn imputation")
trihalomethanes_group = melt(trihalomethanes_group)
diff_triha = ggplot(trihalomethanes_group, aes(x=value, group = variable, fill = variable)) + geom_density(alpha = 0.5)+labs(title = "Distribution of trihalomethanes with and without knn imputation") + scale_fill_manual(values = c("red","blue"))
```

```{r}
plot_grid(diff_ph, diff_sulfate, diff_triha)
```

The distribution does not really change except for 'Sulfate' variable where we had 25% of missing values. In general, our distribution becomes a bit more centered but the impact does not seem that important. It respected the distribution of our variables.

#### 3D graph using ph, organic carbon and Turbidity to see how our potable and non potable water are dispersed.

It represents our water quality based on 'ph', 'Organic_carbon' and 'Turbidity'.
```{r}
fig <- plot_ly(train_scaled_back, x = ~ph, y = ~Turbidity, z = ~Organic_carbon, color = ~Potability)
fig <- fig %>% add_markers()
fig <- fig %>% layout(scene = list(xaxis = list(title = 'ph'),
                     yaxis = list(title = 'Turbidity'),
                     zaxis = list(title = 'Organic_carbon')))
fig
```

We can see that they are all stacked together. It doesn't seem like there's a seperation of the 2 groups. So, it will be hard to classify them correctly using a model. But looking at this graph we can clearly see that our data isn't the best.

#### Implemetation of our models : 

We are going to have as a benchmark the naive rule since it's going to be hard to classify our observations properly. The naive rule is when a model classify all our observation into the most prevalent class. Here, since we have 60.99 % of our observations that are non potable water. The lowest accuracy possible for a model would be to classify every observation into non potable water and get an accuracy of 0.6099. We should be aware of that and at least try to do better then this naive rule.


##### Let's start with KNN : 
For KNN, it is better to have our train and test set that have value ranged from 0 - 1. So, we will use the train range and valid range sets.
First we need to find the best K for our model so let's do a cross-validation with a preprocessing using the range method to make our estimation more robust : 
```{r}
set.seed(1) 

tr_control = trainControl(method = "cv",number = 5 ,summaryFunction =  multiClassSummary) # 5 fold CV

knn_cross_val = train(Potability ~ ., # Predicted class
               data = train_scaled_back,    # on our training data
               method = "knn",    # we use knn
               trControl = tr_control,
               preProcess="range", # preProcess our data with the range method for every fold
               tuneGrid = data.frame(k = seq(50)))
print(knn_cross_val)
knn_cv_acc = knn_cross_val$results[max(knn_cross_val$results$Accuracy) == knn_cross_val$results$Accuracy, c("k","Accuracy")]$Accuracy
knn_cv_acc

knn_cross_val$resample$Accuracy
```

The best k that maximizes our accuracy is k = 18 and the results of the average accuracy of our cross-validation using k = 18 is 0.631. Now that we know the best tune for our model let's use it for our test set.

```{r}
set.seed(1)
library(class)
knn_res = knn(train.range[-10], test.range[-10], cl=train.range$Potability,k=18, prob = T)
confusion_knn = confusionMatrix(knn_res, test.range$Potability,positive = "1")
confusion_knn
accuracy_knn = confusion_knn$overall["Accuracy"]
```

as we can see with knn we predicted poorly our class 0.6305 of accuracy is really not that good. We predicted our class 0 (not potable water) better than potable water. Why ? Well it's probably because we have more non potable water in our data set and having a large k make our prediction based on the more prevalent class which is non potable water.

```{r}
colours = c("#599ad3", "#f9a65a")
res_knn = data.frame("Accuracy" = c(knn_cv_acc, accuracy_knn), "Sets"=c("avg cross-validation","test"))
ggplot(res_knn, aes(x=Sets, y=Accuracy, fill = Sets)) + geom_bar(stat = "identity") + labs(title = "Accuracy CV using training set VS test set") + scale_fill_manual(values = colours) + geom_text(mapping= aes(x = Sets, y=Accuracy,label = round(Accuracy,4)), vjust=-0.5, size = 2, col = "red")
```

As we can see, the accuracy of the average 5 fold cross-validation performed on our training set is very close to the one of our test set. So, I don't think we are overfitting since the accuracy is quite similar for the 2 cases.

### Now let's perform a decision tree : 
for a tree we don't need to work with scaled variables so let's scale back a train and test set:
Scaling back our train and validations sets : 

Building a full grown tree : 
```{r}
set.seed(1)
train_tree = rpart(Potability ~ . , data=train_scaled_back, method = "class", control=rpart.control(maxdepth=30,cp=0, minsplit = 1,minbucket = 1, xval = 5)) 
```

Pruning our full grown tree using 5 fold cross-validation : 
```{r}
library(rattle)
prune_cp.index = which.min(train_tree$cptable[,"xerror"])
prune_cp = train_tree$cptable[prune_cp.index,"CP"]
prune_cp #  0.007822686 CP

pruned_train_tree <- prune(train_tree, cp = prune_cp)
fancyRpartPlot(pruned_train_tree)
```

Looking at the tree, it seems that sulfate is the most important variable followed by harness and Trihalomethanes since they are the first one to start the split of our data.

```{r}
### CV train accuracy : 
classificationtree_train_predict = predict(pruned_train_tree, train_scaled_back , type = "class")
confusionMatrix(classificationtree_train_predict, train_scaled_back$Potability, positive = "1")

#### test accuracy
classificationtree_test_predict = predict(pruned_train_tree, test_scaled_back , type = "class")
classificationtree_test_proba = predict(pruned_train_tree, test_scaled_back , type = "prob")[,2]
confusion_tree = confusionMatrix(classificationtree_test_predict, test_scaled_back$Potability, positive = "1")
confusion_tree
accuracy_tree = confusion_tree$overall["Accuracy"]

train_tree_acc = confusionMatrix(classificationtree_train_predict, train_scaled_back$Potability, positive = "1")$overall["Accuracy"]
```

We can see that with a tree we have an accuracy of 63.44% which is better than with knn ! The balanced accuracy is at 56.9 % meaning that we are doing a little bit better than by guessing randomly. It is clear and normal that we get those kind of results since it seems that the data set was weird. Indeed we had same values for all the variables even if they were potable or non potable water

```{r}
colours = c("#599ad3", "#f9a65a")
tree_res = data.frame("Accuracy" = c(train_tree_acc, accuracy_tree), "Sets"=c("CV pruned tree train","pruned tree test"))
ggplot(tree_res, aes(x=Sets, y=Accuracy, fill = Sets)) + geom_bar(stat = "identity") + labs(title = "Accuracy CV using training set VS test set") + scale_fill_manual(values = colours) + geom_text(mapping= aes(x = Sets, y=Accuracy,label = round(Accuracy,4)), vjust=-0.5, size = 2, col = "red")
```


### Naive bayes : don't need scaling
```{r}
library(e1071)
nbayes.model = naiveBayes(Potability ~ ., data = train_scaled_back)
nbayes.predict = predict(nbayes.model, test_scaled_back)
nbayes_predict_proba = predict(nbayes.model, test_scaled_back,type = "raw")[,2]
confusion_nb = confusionMatrix(nbayes.predict, test_scaled_back$Potability, positive = "1")
accuracy_nb = confusion_nb$overall["Accuracy"]
confusion_nb
accuracy_nb
confusionMatrix(predict(nbayes.model, train_scaled_back), train_scaled_back$Potability, positive = "1")
```

Accuracy : 0.6237, Balanced Accuracy : 0.555 but we still struggle to predict well.

```{r, echo=FALSE,warning=FALSE,message=FALSE,error=FALSE, results='hide',fig.keep='all'}
set.seed(1) 

tr_control = trainControl(method = "cv",number = 5) # 5 fold CV

nb_cv = train(Potability ~ ., # Predicted class
               data = train_scaled_back,    # on our training data
               method = "nb",    # we use knn
               trControl = tr_control,
              )
               
confusionMatrix(nb_cv)
nb_cv_acc = nb_cv$results[1,"Accuracy"]
#knn_cv_acc = knn_cross_val$results[max(knn_cross_val$results$Accuracy) == knn_cross_val$results$Accuracy, #c("k","Accuracy")]$Accuracy
#knn_cv_acc

#knn_cross_val$resample$Accuracy
```

```{r}
colours = c("#599ad3", "#f9a65a")
nb_res = data.frame("Accuracy" = c(nb_cv_acc, accuracy_nb), "Sets"=c("Avg 5 fold CV train","test"))
ggplot(nb_res, aes(x=Sets, y=Accuracy, fill = Sets)) + geom_bar(stat = "identity") + labs(title = "Accuracy CV using training set VS test set") + scale_fill_manual(values = colours) + geom_text(mapping= aes(x = Sets, y=Accuracy,label = round(Accuracy,4)), vjust=-0.5, size = 2, col = "red")
```

Here using NB cross-validation or just using simply naive bayes gives us the same results on our test set.


### Boosting : 


```{r}
set.seed(1) 
library(fastAdaboost)
library(adabag)
tr_control = trainControl(method = "cv",number = 5 ,summaryFunction =  multiClassSummary) 

boosted_cv = train(Potability ~ ., #. is to tell that we use all our variables as predictors on personal loan
               data = train_scaled_back,    ## on our training data
               method = "adaboost",    ### we use classification tree
             
               trControl = tr_control)
print(boosted_cv)
```

Accuracy was used to select the optimal model using the largest value. CV Accuracy = 0.6337712
The final values used for the model were nIter = 150 (150 trees) and method = Adaboost.M1.

Let's use our cv model to predict our test data.
```{r}
set.seed(1)
cv_boost_acc = 0.6337712

test_predict_boost = predict(boosted_cv, test_scaled_back)
test_predict_boost_proba = predict(boosted_cv, test_scaled_back , type = "prob")[,2]
confusion_boost = confusionMatrix(as.factor(test_predict_boost), test_scaled_back$Potability, positive = "1")
confusion_boost
accuracy_boost = confusion_boost$overall["Accuracy"]

```


```{r}
plot(varImp(boosted_cv), main="Variable Importance with boosting")
```

Sulfate, ph and Harness are the 3 most important variables in this model.

Accuracy : 0.642,Balanced Accuracy : 0.6025    Already better. 
```{r}
colours = c("#599ad3", "#f9a65a")
boost_res = data.frame("Accuracy" = c(cv_boost_acc, accuracy_boost), "Sets"=c("Avg 5 fold CV train","test"))
ggplot(boost_res, aes(x=Sets, y=Accuracy, fill = Sets)) + geom_bar(stat = "identity") + labs(title = "Accuracy CV using training set VS test set") + scale_fill_manual(values = colours) + geom_text(mapping= aes(x = Sets, y=Accuracy,label = round(Accuracy,4)), vjust=-0.5, size = 2, col = "red")
```

### bagging

```{r}
set.seed(1)
tr_control = trainControl(method = "cv",number = 5 ,summaryFunction =  multiClassSummary) 

bagged_cv = train(Potability ~ ., #. is to tell that we use all our variables as predictors on personal loan
               data = train_scaled_back,    ## on our training data
               method = "treebag",    ### we use classification tree
               nbagg = 200,
               trControl = tr_control
              )
print(bagged_cv)
cv_bagged_acc = bagged_cv$results$Accuracy
plot(varImp(bagged_cv), main="Variable Importance with Bagging")
```



Unfortunately there's no tuning parameter for this method. (But Bagged AdaBoost has ! )
Accuracy of the 5 fold cv = 0.6541236 using 200 trees

Now let's look on our test data.
```{r}
set.seed(1)

test_predict_bag = predict(bagged_cv, test_scaled_back , type = "raw")
test_predict_bag_prob = predict(bagged_cv, test_scaled_back , type = "prob")[,2]
### to get the class, we need to use $class
#valid_predict_boost
confusion_bag = confusionMatrix(as.factor(test_predict_bag), test_scaled_back$Potability, positive = "1")
confusion_bag
accuracy_bag = confusion_bag$overall["Accuracy"]
```

```{r}
colours = c("#599ad3", "#f9a65a")
bag_res = data.frame("Accuracy" = c(cv_bagged_acc, accuracy_bag), "Sets"=c("Avg 5 fold CV train","test"))
ggplot(bag_res, aes(x=Sets, y=Accuracy, fill = Sets)) + geom_bar(stat = "identity") + labs(title = "Accuracy CV using training set VS test set") + scale_fill_manual(values = colours) + geom_text(mapping= aes(x = Sets, y=Accuracy,label = round(Accuracy,4)), vjust=-0.5, size = 2, col = "red")
```

Accuracy : 0.6687 Balanced Accuracy : 0.6187

#### Random Forest
CV

```{r}
library(randomForest)
set.seed(1)
control = trainControl(method='cv', 
                        number=5,
                        search='grid')

tunegrid = expand.grid(.mtry = (1:9)) 

rf_gridsearch = train(Potability ~ ., 
                       data = train_scaled_back,
                       method = 'rf',
                       metric = 'Accuracy',
                       tuneGrid = tunegrid,
                       trControl = control,
                       importance = T)
print(rf_gridsearch)

```

When using cross validation on random forest using the scaled back data, the best mtry to use is 6. We get a 0.6597 accuracy.

```{r}
library(randomForest)
rf_cv_acc = 0.6597099
set.seed(1)
randomf = randomForest(Potability~., data = train_scaled_back,mtry = 6, importance = T )


test_predict_rf = predict(randomf, test_scaled_back , type = "class")
test_predict_rf_proba = predict(randomf, test_scaled_back , type = "prob")[,2]

confusion_rf = confusionMatrix(as.factor(test_predict_rf), test_scaled_back$Potability, positive = "1")
confusion_rf
accuracy_rf = confusion_rf$overall["Accuracy"]
varImpPlot(randomf)
```

Accuracy : 0.674
Balanced Accuracy : 0.6228  

the 3 most important variables are ph, Sulfate and Hardness.

```{r}
colours = c("#599ad3", "#f9a65a")
rf_res = data.frame("Accuracy" = c(rf_cv_acc, accuracy_rf), "Sets"=c("Avg 5 fold CV train","test"))
ggplot(rf_res, aes(x=Sets, y=Accuracy, fill = Sets)) + geom_bar(stat = "identity") + labs(title = "Accuracy CV using training set VS test set") + scale_fill_manual(values = colours) + geom_text(mapping= aes(x = Sets, y=Accuracy,label = round(Accuracy,4)), vjust=-0.5, size = 2, col = "red")
```

#### Logistic Regression

```{r}
set.seed(1)
train_control = trainControl(method = "cv", number = 5)

# train the model on training set
logi_cv = train(Potability ~.,
               data = train_scaled_back,
               trControl = train_control,
               method = "glm",
               family=binomial())

# print cv scores
summary(logi_cv)
print(logi_cv)
confusionMatrix(logi_cv)

```

No variables are significant.
0.6093608 accuracy. Here are model classify almost all our classes as 0 since it's the predominant class to maximize its accuracy. Unfortunately, this doesn't give us any information ! Here, it does even worse then then naive rule. Naive rule acc = 0.6099 VS 0.6094. This model classified 0.1% of our data into 1 while it was 0.

```{r}
logi_cv_acc = logi_cv$results$Accuracy
pred.logit.reg = predict(logi_cv, test_scaled_back[,-10], type = "prob")[,2]
logi_pred_class = predict(logi_cv, test_scaled_back[,-10], type = "raw")
confusion_logi = confusionMatrix(predict(logi_cv, test_scaled_back), test_scaled_back$Potability, positive = "1")
confusion_logi
accuracy_logi = confusion_logi$overall["Accuracy"]
```

as you can see its does the same for our test data. this is a pure example of the naive rule where the model classify every observations into the most prevalent class.


Let's visualize our results :

```{r}
#library(devtools)
#devtools::install_github('cttobin/ggthemr')

ConfusionMatrixInfo <- function( data, predict, actual, cutoff )
{   
    # extract the column ;
    # relevel making 1 appears on the more commonly seen position in 
    # a two by two confusion matrix 
    predict <- data[[predict]]
    actual  <- relevel( as.factor( data[[actual]] ), "1" )
    
    result <- data.table( actual = actual, predict = predict )

    # caculating each pred falls into which category for the confusion matrix
    result[ , type := ifelse( predict >= cutoff & actual == 1, "TP",
                      ifelse( predict >= cutoff & actual == 0, "FP", 
                      ifelse( predict <  cutoff & actual == 1, "FN", "TN" ) ) ) %>% as.factor() ]

    # jittering : can spread the points along the x axis 
    plot <- ggplot( result, aes( actual, predict, color = type ) ) + 
            geom_violin( fill = "white", color = NA ) +
            geom_jitter( shape = 1 ) + 
            geom_hline( yintercept = cutoff, color = "blue", alpha = 0.6 ) + 
            scale_y_continuous( limits = c( 0, 1 ) ) + 
            scale_color_discrete( breaks = c( "TP", "FN", "FP", "TN" ) ) + # ordering of the legend 
            guides( col = guide_legend( nrow = 2 ) ) + # adjust the legend to have two rows  
            ggtitle( sprintf( "Confusion Matrix with Cutoff at %.2f", cutoff ) )

    return( list( data = result, plot = plot ) )
}
library(data.table)
library(ggthemr)
logi_data = data.frame("prediction" = predict(logi_cv, test_scaled_back[,-10], type = "prob")[,2], "True" = test_scaled_back$Potability )
cm_info <- ConfusionMatrixInfo( data = logi_data, predict = "prediction", 
                                actual = "True", cutoff = .5 )
ggthemr("flat")
cm_info$plot
```

We can clearly see that it's not very good. We are doing as good as the baseline (the naive rule). This model give us no real information ! 

```{r}
colours = c("#599ad3", "#f9a65a")
logi_res = data.frame("Accuracy" = c(logi_cv_acc, accuracy_logi), "Sets"=c("Avg 5 fold CV train","test"))
ggplot(logi_res, aes(x=Sets, y=Accuracy, fill = Sets)) + geom_bar(stat = "identity") + labs(title = "Accuracy CV using training set VS test set") + scale_fill_manual(values = colours) + geom_text(mapping= aes(x = Sets, y=Accuracy,label = round(Accuracy,4)), vjust=-0.5, size = 2, col = "red")
```

#### Neural nets : Here we need to use the train set and validation set that has been preprocessed using the range method. Also, since our variable were symmetrical we don't need to use the log transformation.

```{r}
set.seed(1)
control <- trainControl(method='cv', 
                        number=5)

neural.net_cv <- train(Potability ~ ., data = train_scaled_back,
    method = "nnet",  trControl = control, preProcess = "range")

neural.net_cv$bestTune
neural.net_cv
nn_cv_acc = max(neural.net_cv$results$Accuracy)
confusionMatrix(neural.net_cv)
```

Using Cross-validation, we should use 5 nodes in 1 hidden layer and decay = 0.1 is the regularization parameter to avoid over-fitting

```{r}
nn_predict = predict(neural.net_cv$finalModel, test.range, type = "class")
nn_predict_proba = predict(neural.net_cv$finalModel, test.range, type = "raw")
confusion_nn_cv =confusionMatrix(as.factor(nn_predict), test.range$Potability, positive = "1")
confusion_nn_cv
accuracy_nn = confusionMatrix(as.factor(nn_predict), test.range$Potability, positive = "1")$overall["Accuracy"]
#train_predict_nn = predict(neural.net_cv, train.range) #Check if we were overfitting but not at all ! 
#confusionMatrix(train_predict_nn, train.range$Potability, positive = "1")
```



Accuracy : 0.6855 which is the best we have seen so far ! 

```{r}
colours = c("#599ad3", "#f9a65a")
nn_res = data.frame("Accuracy" = c(nn_cv_acc, accuracy_nn), "Sets"=c("Avg 5 fold CV train","test"))
ggplot(nn_res, aes(x=Sets, y=Accuracy, fill = Sets)) + geom_bar(stat = "identity") + labs(title = "Accuracy CV using training set VS test set") + scale_fill_manual(values = colours) + geom_text(mapping= aes(x = Sets, y=Accuracy,label = round(Accuracy,4)), vjust=-0.5, size = 2, col = "red")
```


#### Creating a data frame storing all our result from the test set.

```{r}
accuracy_all = data.frame("Model" = 
                            c("K-NN","Naive-Bayes","logistic","Decision_tree","Bagging","Boosting","Random_forest","Neural_nets"),
                          "Accuracy"=c(accuracy_knn, accuracy_nb, accuracy_logi, accuracy_tree, accuracy_bag, accuracy_boost, accuracy_rf, accuracy_nn))
ggplot(accuracy_all, aes(x=Model, y = Accuracy, fill = Accuracy)) + geom_bar(width = 0.8,stat="identity") + scale_fill_viridis_c() + geom_text(mapping= aes(x = Model, y=Accuracy,label = round(Accuracy,4)), vjust=-0.5, size = 2, col = "red") + labs(title = "Accuracy of our models on the Test set")  + theme_pubr()+theme(axis.text.x = element_text(angle=90, vjust=1))
```

```{r}
proba_weird = attr(knn_res,"prob") # Returns only the highest proba so I will compare the proba to the outcome to find if it's the proba of being 1 or 0 ...
proba_new_knn = proba_weird + as.numeric(as.character(knn_res))
#proba_new_knn

proba_knn = ifelse(proba_new_knn > 1, proba_new_knn-1, 1-proba_new_knn)


results_all = data.frame("KNN_pred" = knn_res, "KNN_prob" = proba_knn, "logi_pred"=logi_pred_class, "logi_prob" = pred.logit.reg, "NB_pred" = nbayes.predict, "NB_prob" = nbayes_predict_proba, "CT_pred" = classificationtree_test_predict,"CT_prob" = classificationtree_test_proba, "bag_pred" = test_predict_bag, "bag_prob" = test_predict_bag_prob, "boost_pred" =test_predict_boost, "boost_prob" = test_predict_boost_proba, "rf_pred" = test_predict_rf, "rf_prob" = test_predict_rf_proba, "neural_net_pred"=nn_predict, "neural_net_prob" = nn_predict_proba)
head(results_all)
```

Ensemble method : 
```{r}
### Average prediction : WE have 8 models : knn, logi, naive bayes, CT, bag, boost, rf, neural net
ensemble_pred = as.numeric(as.character(results_all$KNN_pred)) + as.numeric(as.character(results_all$logi_pred)) + as.numeric(as.character(results_all$NB_pred)) + as.numeric(as.character(results_all$CT_pred)) +
  as.numeric(as.character(results_all$bag_pred)) + as.numeric(as.character(results_all$boost_pred)) + as.numeric(as.character(results_all$rf_pred)) + as.numeric(as.character(results_all$neural_net_pred))

ensemble_majority = ifelse(ensemble_pred>= 4,1,0)
results_all$ensemble_majority = ensemble_majority


### Average proba : 
results_all$ensemble_proba = ifelse((results_all$KNN_prob +  results_all$logi_prob +  results_all$NB_prob +  results_all$CT_prob +  results_all$bag_prob +  results_all$boost_prob +  results_all$rf_prob +  results_all$neural_net_prob)/8 >= 0.5, 1,0)

results_all$true = test_scaled_back$Potability
```


```{r}
confu_majority = confusionMatrix(as.factor(results_all$ensemble_majority), test_scaled_back$Potability,positive = "1")
confu_majority
confu_avg_proba = confusionMatrix(as.factor(results_all$ensemble_proba), test_scaled_back$Potability,positive = "1")
confu_avg_proba
```

```{r}
acc_majority = confu_majority$overall["Accuracy"]
acc_avg_proba = confu_avg_proba$overall["Accuracy"]

ensemble_df = data.frame("Accuracy"=c(acc_majority,acc_avg_proba),"Ensemble"=c("Majority","Average Proba"))

ggplot(ensemble_df, aes(x = Ensemble, y=Accuracy, fill = Ensemble)) + geom_bar(stat="identity") + geom_text(mapping= aes(x = Ensemble, y=Accuracy,label = round(Accuracy,4)), vjust=-0.5, size = 2, col = "red") + labs(title = "Accuracy of our Ensemble models on the Test set") + theme_bw()
```


Ensemble average seems to perform better 0.6733 Accuracy. As we can see we have a hard time predicting the Potable water. It is probably because they both have the same distribution of each feature and that we have more non potable water in our data set. Also having some non potable water that has the same distribution as potable water make it hard for any of our model to find which class it belongs to. 














