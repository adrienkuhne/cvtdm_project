---
title: "Predicting Water Potability"
author: "Romain Kursner & Adrien Kühne"
date: "11/18/2021"
output: html_document
---

### Loading the data set : 
```{r}
rm(list = ls())
df = read.csv("water_potability.csv")
```

### Loading our libraries :
```{r}
library(naniar) #### missing values graph
library(ggplot2) ### plotting library
library(tidyverse)
library(plotly) #### nice interactive graph
library(rpart)
library(rpart.plot)
library(forecast) ### forcast accuracy
library(caret) ### confusion matrix, CV, preprocess and more
library(cowplot) ## multiple ggplot subplot
library(hrbrthemes) ### nice ggplot themes
library(mice) ### missing valiues 
library(pheatmap) ### Nice heatmaps
library(fmsb) ### Radar chart
```

### Exploratory data analysis :

```{r}
str(df)
```

As you can see, our data set has 3276 observations and 10 variables. They are all numerical values except 'Potability' which is an integer. It is normal since 'Potability' indicates if our water is either potable or not. We will need to transform it into a factor later on. We can already see some missing values.

```{r}
summary(df)
```

Here we can see the minimum, maximum, mean and median of all our variables. What is really important to notice is that we have 491 missing values in 'ph', 781 in 'Sulfate' and 162 in 'Trihalomethanes.' We will need to find a good strategy to deal with them.

We could check if we have a balanced data since we are going to classify our water based on the 'Potability' variable.

```{r}
potable_amount = sum(df$Potability == 1)/nrow(df)
notpotable_amount = sum(df$Potability == 0) / nrow(df)
pot_perc = paste0(round(potable_amount,4)*100, "% : Potable")
notpot_perc = paste0(round(notpotable_amount,4)*100, "% : Non-Potable")

amount = data.frame("Group" =c("Potable","Non-Potable"),"Percentage"=c(potable_amount, notpotable_amount), "Labels" = c(pot_perc,notpot_perc))

pie_chart = ggplot(amount, aes(x = "", y = Percentage, fill = Group)) +
  geom_col(color = "black") +
  geom_label(aes(label = Labels), color = c(1, "white"),
            position = position_stack(vjust = 0.5),
            show.legend = FALSE) +
  guides(fill = guide_legend(title = "Quality of the Water"))+
  scale_fill_viridis_d() +
  coord_polar(theta = "y") + labs(title = "Proportion of Potable and Non-Potable Water") +
  theme_void()

amount$total = c(sum(df$Potability == 1),sum(df$Potability == 0))

bar_chart = ggplot(amount, aes(x = Group, y = total, fill = Group)) + geom_bar(stat = "identity") +
  scale_fill_viridis_d() + labs(title = "Quantity of good and bad water in our data set", x = "Water Quality", y = "Total amount", fill = "Water Quality")

plot_grid(pie_chart, bar_chart, labels = "AUTO")

```

We can see that we have more non-potable than potable water. In general, even if we have more non-potable water, our data set is pretty balanced. We have 1278 Potable and 1998 non-potable water. I think we can focus our analysis on accuracy and F1 score to be more robust.

### Distribution of all our variables by using some boxplots : 

```{r, fig.height=9, fig.width=9}
par(mfrow=c(3,3))
for (i in seq(1:9)) {
  avg_0 = mean(df[df$Potability == 0, i],na.rm = TRUE)
  avg_1 = mean(df[df$Potability == 1, i],na.rm = TRUE)
  means = c(avg_0, avg_1)
  boxplot(df[,i]~df$Potability, main = paste(colnames(df[i])), xlab = "Water Quality", ylab = colnames(df[i]), col = c("#69b3a2", "#404080"))
  points(1:2,means, col = "red", pch = 19)
  legend("topright",legend="Means", col = "red", pch =19)
  
}
```

The distribution of both groups of water seems pretty similar. But we can clearly see something weird going on. First I would think that there would be a difference between the 2 groups. Second, the who recommends that the desirable limit of 'Solids' is 500 mg/liter and maximum limit of 'Solids' is 1000mg/liter (WHO) but we can clearly see that we have potable water that are well above this recommendation.

### Distribution of all our variables by using some histograms : 

```{r}
ggplot(df, aes(x=ph, fill=factor(Potability))) +
    geom_histogram( color="#e9ecef", alpha=0.6, position = 'identity') +
    scale_fill_manual(values=c("#69b3a2", "#404080"), labels = c("Non-potable","Potable")) +
    geom_vline(aes(xintercept = median(df$ph, na.rm=T), color="median")) +
    geom_vline(aes(xintercept = mean(df$ph,na.rm = T), color ="mean")) +
    geom_vline(aes(xintercept =6.5, color="WHO recommendation")) +
    geom_vline(aes(xintercept =8.5, color="WHO recommendation")) +
    scale_color_manual(values = c("red","blue", "black"))  +
    theme_ipsum() +
    labs(fill="Quality of the water", title= "Distribution of ph for potable and non-potable water", color = "Mean and median")
```

The 'ph' variable seems pretty normally distributed. We can see that the 'ph' variable is close to symmetric (median=mean). The potable and non potable water seem to be normally distributed with pretty much the same value for ph which seems a little bit weird. The WHO recommends to get drink water with a pH from 6.5 to 8.5.  

```{r}
ggplot(df, aes(x=Hardness, fill=factor(Potability))) +
    geom_histogram( color="#e9ecef", alpha=0.6, position = 'identity') +
    scale_fill_manual(values=c("#69b3a2", "#404080"), labels = c("Non-potable","Potable")) +
    geom_vline(aes(xintercept = median(df$Hardness, na.rm=T), color="median")) +
    geom_vline(aes(xintercept = mean(df$Hardness,na.rm = T), color ="mean")) +
    scale_color_manual(values = c("red","blue"))  +
    theme_ipsum() +
    labs(fill="Quality of the water", title= "Distribution of Hardness for potable and non-potable water", color = "Mean and median")
#median(df$Hardness, na.rm=T)
#mean(df$Hardness,na.rm = T)
```

The 'Hardness' variable seems pretty normally distributed. We can see that the 'Hardness' variable is close to symmetric (median=mean).It seems more centered as well. The WHO does not recommend a limit of 'Hardness'. In drinking-water, hardness is in the range 10–500 mg of calcium carbonate per litre source : https://www.who.int/water_sanitation_health/dwq/hardness.pdf


```{r}
ggplot(df, aes(x=Solids, fill=factor(Potability))) +
    geom_histogram( color="#e9ecef", alpha=0.6, position = 'identity') +
    scale_fill_manual(values=c("#69b3a2", "#404080"), labels = c("Non-potable","Potable")) +
    geom_vline(aes(xintercept = median(df$Solids, na.rm=T), color="median")) +
    geom_vline(aes(xintercept = mean(df$Solids,na.rm = T), color ="mean")) +
    geom_vline(aes(xintercept = 500, color ="WHO recommendation")) +
    geom_vline(aes(xintercept = 1000, color ="WHO recommendation")) +
    scale_color_manual(values = c("red","blue", "black"))  +
    theme_ipsum() +
    labs(fill="Quality of the water", title= "Distribution of Solids for potable and non-potable water", color = "Mean and median")

```

The 'Solids' variable seems pretty normally distributed. We can see that the 'Solids' variable is not close to symmetric and is right-skewed (median<mean). The desirable limit of 'Solids' is 500 mg/liter and maximum limit of 'Solids' is 1000mg/liter (WHO). Once again, it seems that our values are not realistic for potable water.

```{r}
ggplot(df, aes(x=Chloramines, fill=factor(Potability))) +
    geom_histogram( color="#e9ecef", alpha=0.6, position = 'identity') +
    scale_fill_manual(values=c("#69b3a2", "#404080"), labels = c("Non-potable","Potable")) +
    geom_vline(aes(xintercept = median(df$Chloramines, na.rm=T), color="median")) +
    geom_vline(aes(xintercept = mean(df$Chloramines,na.rm = T), color ="mean")) +
    geom_vline(aes(xintercept = 0, color ="WHO recommendation")) +
    geom_vline(aes(xintercept = 4, color ="WHO recommendation")) +
    scale_color_manual(values = c("red","blue","black"))  +
    theme_ipsum() +
    labs(fill="Quality of the water", title= "Distribution of Chloramines for potable and non-potable water", color = "Mean and median")

```

The 'Chloramines' variable seems pretty normally distributed. We can see that the 'Chloramines' variable is close to symmetric (median=mean). It seems more centered as well. The desirable limit of 'Chloramines' is 4 mg/liter (WHO). Once again, it seems that our values are not realistic.

```{r}
ggplot(df, aes(x=Sulfate, fill=factor(Potability))) +
    geom_histogram( color="#e9ecef", alpha=0.6, position = 'identity') +
    scale_fill_manual(values=c("#69b3a2", "#404080"), labels = c("Non-potable","Potable")) +
    geom_vline(aes(xintercept = median(df$Sulfate, na.rm=T), color="median")) +
    geom_vline(aes(xintercept = mean(df$Sulfate,na.rm = T), color ="mean")) +
    scale_color_manual(values = c("red","blue"))  +
    theme_ipsum() +
    labs(fill="Quality of the water", title= "Distribution of Sulfate for potable and non-potable water", color = "Mean and median")

```

The 'Sulfate' variable seems pretty normally distributed. We can see that the 'Sulfate' variable is close to symmetric (median=mean).It seems more centered as well. The WHO recommend a level of 500 mg/l as a limit for sulfate.

```{r}
ggplot(df, aes(x=Conductivity, fill=factor(Potability))) +
    geom_histogram( color="#e9ecef", alpha=0.6, position = 'identity') +
    scale_fill_manual(values=c("#69b3a2", "#404080"), labels = c("Non-potable","Potable")) +
    geom_vline(aes(xintercept = median(df$Conductivity, na.rm=T), color="median")) +
    geom_vline(aes(xintercept = mean(df$Conductivity,na.rm = T), color ="mean")) +
    geom_vline(aes(xintercept = 0, color ="WHO recommendation")) +
    geom_vline(aes(xintercept = 400, color ="WHO recommendation")) +
    scale_color_manual(values = c("red","blue", "black"))  +
    theme_ipsum() +
    labs(fill="Quality of the water", title= "Distribution of Conductivity for potable and non-potable water", color = "Mean and median")

```

The 'Conductivity' variable seems pretty normally distributed. Even though the 'Conductivity' variable is slightly right-skewed (median<mean), we can see that it is close to symmetric (median=mean).The desirable limit of 'Conductivity' should not exceed 400 μS/cm (WHO). 

```{r}
ggplot(df, aes(x=Organic_carbon, fill=factor(Potability))) +
    geom_histogram( color="#e9ecef", alpha=0.6, position = 'identity') +
    scale_fill_manual(values=c("#69b3a2", "#404080"), labels = c("Non-potable","Potable")) +
    geom_vline(aes(xintercept = median(df$Organic_carbon, na.rm=T), color="median")) +
    geom_vline(aes(xintercept = mean(df$Organic_carbon,na.rm = T), color ="mean")) +
  geom_vline(aes(xintercept = 2, color ="Recommendation")) +
    scale_color_manual(values = c("red","blue", "black"))  +
    theme_ipsum() +
    labs(fill="Quality of the water", title= "Distribution of Organic carbon for potable and non-potable water", color = "Mean and median")

```

The 'Organic_carbon' variable seems pretty normally distributed. We can see that the 'Organic_carbon' variable is close to symmetric (median=mean). The recommendation is that water should note have higher than 2mg/L of organic carbon to be drinkable. https://www.canada.ca/en/health-canada/programs/consultation-organic-matter-drinking-water/document.html

```{r}
ggplot(df, aes(x=Trihalomethanes, fill=factor(Potability))) +
    geom_histogram( color="#e9ecef", alpha=0.6, position = 'identity') +
    scale_fill_manual(values=c("#69b3a2", "#404080"), labels = c("Non-potable","Potable")) +
    geom_vline(aes(xintercept = median(df$Trihalomethanes, na.rm=T), color="median")) +
    geom_vline(aes(xintercept = mean(df$Trihalomethanes,na.rm = T), color ="mean")) +
    geom_vline(aes(xintercept = 0, color ="WHO recommendation")) +
    geom_vline(aes(xintercept = 80, color ="WHO recommendation")) +
    scale_color_manual(values = c("red","blue","black"))  +
    theme_ipsum() +
    labs(fill="Quality of the water", title= "Distribution of Trihalomethanes for potable and non-potable water", color = "Mean and median")

```

The 'Trihalomethanes' variable seems pretty normally distributed. We can see that the 'Trihalomethanes' variable is close to symmetric (median=mean). The desirable limit of 'Trihalomethanes' is 80 ppm.

```{r}
ggplot(df, aes(x=Turbidity, fill=factor(Potability))) +
    geom_histogram( color="#e9ecef", alpha=0.6, position = 'identity') +
    scale_fill_manual(values=c("#69b3a2", "#404080"), labels = c("Non-potable","Potable")) +
    geom_vline(aes(xintercept = median(df$Turbidity, na.rm=T), color="median")) +
    geom_vline(aes(xintercept = mean(df$Turbidity,na.rm = T), color ="mean")) +
    geom_vline(aes(xintercept = 0, color ="WHO recommendation")) +
    geom_vline(aes(xintercept = 5, color ="WHO recommendation")) +
    scale_color_manual(values = c("red","blue","black"))  +
    theme_ipsum() +
    labs(fill="Quality of the water", title= "Distribution of Turbidity for potable and non-potable water", color = "Mean and median")

```

The 'Turbidity' variable seems pretty normally distributed. We can see that the 'Turbidity' variable is close to symmetric (median=mean). The desirable limit of 'Turbidity' is 5 NTU (WHO).

### Difference between our potable and non-potable water by using a radar chart.

```{r, fig.width=8, fig.height=8}

par(mfrow=c(1,2))

df_1 = df[df$Potability == 1,]
df_0 = df[df$Potability == 0,]

#### to to put max min and the value

rad_1 = data.frame(ph = c(max(df_1$ph, na.rm = T),0,mean(df_1$ph, na.rm=T)),
                   Hardness = c(max(df_1$Hardness),0,mean(df_1$Hardness, na.rm=T)),
                   Solids = c(max(df_1$Solids),0,mean(df_1$Solids, na.rm=T)),
                   Chloramines = c(max(df_1$Chloramines),0,mean(df_1$Chloramines, na.rm=T)),
                   Sulfate = c(max(df_1$Sulfate, na.rm = T),0,mean(df_1$Sulfate, na.rm=T)),
                   Conductivity = c(max(df_1$Conductivity),0,mean(df_1$Conductivity, na.rm=T)),
                   Organic_Carbon = c(max(df_1$Organic_carbon),0,mean(df_1$Organic_carbon, na.rm=T)),
                   Trihalomethanes = c(max(df_1$Trihalomethanes,na.rm=T),0,mean(df_1$Trihalomethanes, na.rm=T)),
                   Turbidity = c(max(df_1$Turbidity),0,mean(df_1$Turbidity, na.rm=T))
  
                   )

rad_2 = data.frame(ph = c(max(df_0$ph, na.rm = T),0,mean(df_0$ph, na.rm=T)),
                   Hardness = c(max(df_0$Hardness),0,mean(df_0$Hardness, na.rm=T)),
                   Solids = c(max(df_0$Solids),0,mean(df_0$Solids, na.rm=T)),
                   Chloramines = c(max(df_0$Chloramines),0,mean(df_0$Chloramines, na.rm=T)),
                   Sulfate = c(max(df_0$Sulfate, na.rm = T),0,mean(df_0$Sulfate, na.rm=T)),
                   Conductivity = c(max(df_0$Conductivity),0,mean(df_0$Conductivity, na.rm=T)),
                   Organic_Carbon = c(max(df_0$Organic_carbon),0,mean(df_0$Organic_carbon, na.rm=T)),
                   Trihalomethanes = c(max(df_0$Trihalomethanes,na.rm=T),0,mean(df_0$Trihalomethanes, na.rm=T)),
                   Turbidity = c(max(df_0$Turbidity),0,mean(df_0$Turbidity, na.rm=T))

                   )

# Color vector
colors_border=c( rgb(0.2,0.5,0.5,0.9), rgb(0.8,0.2,0.5,0.9) , rgb(0.7,0.5,0.1,0.9) )
colors_in=c( rgb(0.2,0.5,0.5,0.4), rgb(0.8,0.2,0.5,0.4) , rgb(0.7,0.5,0.1,0.4) )

radarchart(rad_1  , axistype=1 ,
 #custom polygon
    pcol=colors_border , pfcol=colors_in , plwd=4 , plty=1,
    #custom the grid
    cglcol="grey", cglty=1, axislabcol="grey", cglwd=0.8,
    #custom labels
    vlcex=0.8 , title = "Potable Water"
    
    )

radarchart(rad_2  , axistype=1 ,
 #custom polygon
    pcol=colors_border , pfcol=colors_in , plwd=4 , plty=1,
    #custom the grid
    cglcol="grey", cglty=1, axislabcol="grey", cglwd=0.8,
    #custom labels
    vlcex=0.8 , title = "Not Potable Water"
    
    )
```

We used the maximum values and 0 when potability = 1 and potability = 0 as the range of the radar chart. We used the mean of each feature for potability = 1 and potability = 0 to represent their characteristics. We can see that there's not a significant difference between them.

```{r}
new_rad = data.frame(ph = c(max(df$ph, na.rm = T),0,mean(df_0$ph, na.rm=T),mean(df_1$ph, na.rm=T)),
                   Hardness = c(max(df_0$Hardness),0,mean(df_0$Hardness, na.rm=T),mean(df_1$Hardness, na.rm=T)),
                   Solids = c(max(df_0$Solids),0,mean(df_0$Solids, na.rm=T),mean(df_1$Solids, na.rm=T)),
                   Chloramines = c(max(df_0$Chloramines),0,mean(df_0$Chloramines, na.rm=T),mean(df_1$Chloramines, na.rm=T)),
                   Sulfate = c(max(df_0$Sulfate, na.rm = T),0,mean(df_0$Sulfate, na.rm=T),mean(df_1$Sulfate, na.rm=T)),
                   Conductivity = c(max(df_0$Conductivity),0,mean(df_0$Conductivity, na.rm=T),mean(df_1$Conductivity, na.rm=T)),
                   Organic_Carbon = c(max(df_0$Organic_carbon),0,mean(df_0$Organic_carbon, na.rm=T),mean(df_1$Organic_carbon, na.rm=T)),
                   Trihalomethanes = c(max(df_0$Trihalomethanes,na.rm=T),0,mean(df_0$Trihalomethanes, na.rm=T),mean(df_1$Trihalomethanes,na.rm=T)),
                   Turbidity = c(max(df_0$Turbidity),0,mean(df_0$Turbidity, na.rm=T),mean(df_1$Turbidity, na.rm=T))

                   )

row.names(new_rad)[3:4] = c("Not Potable", "Potable")


# Color vector
colors_border=c( rgb(0.2,0.5,0.5,0.9), rgb(0.8,0.2,0.5,0.9) )
colors_in=c( rgb(0.2,0.5,0.5,0.4), rgb(0.8,0.2,0.5,0.4))

# plot with default options:
radarchart( new_rad  , axistype=1 , 
    #custom polygon
    pcol=colors_border , pfcol=colors_in , plwd=4 , plty=1,
    #custom the grid
    cglcol="grey", cglty=1, axislabcol="grey", cglwd=0.8,
    #custom labels
    vlcex=0.8 
    )

# Add a legend
legend(x=0.7, y=1, legend = rownames(new_rad[-c(1,2),]), bty = "n", pch=20 , col=colors_in , text.col = "grey", cex=1.2, pt.cex=3)
new_rad

```

In fact, we can see that there is no significant difference between potable and non-potable water. It does not seem realistic at all ! We would expect at least differences between some features.

### Correlation of our variables : 

```{r}
pheatmap(cor(df, use = "complete"), display_numbers = T,cluster_rows = F, cluster_cols = F, fontsize_number = 15)
```

They are not correlated at all between each other. It means that we will not have issues of multicolinearity.

#### Missing values :

```{r}
gg_miss_var(df, show_pct = TRUE)
```

Our initial dataset has 3276 rows and 10 features. It has 1278 observations of potable water and 1998 of non-potable water.
If we drop the missing values, our dataset has 2011 rows and 10 features. It has 811 observations of potable water and 1200 of non-potable water. We would lose 1265 observations. It seems that we have more missing values for the non-potable water.
We can see that almost 25% of the 'Sulfate' and 15% of 'ph' observations have missing values. This is a huge proportion. In comparison, we note that only 5% of the 'Trihalomethanes' observations have missing value. We could check whether the missing values follow a pattern.

#### Potential pattern of the missing values :

```{r}
library(missRanger)
library(dlookr)
plot_na_intersect(df)
```

```{r}
plot_na_pareto(df)
```

```{r}
md.pattern(df,rotate.names = TRUE, plot=TRUE)
```

```{r}
library(visdat)
vis_miss(df)
```

Looking at all the previous graphs, it seems that the missing observations do not really follow a pattern. It does not seem that the NAs are missing in a systematic way.

#### Comparing different methods to find the best one to deal with the missing values. To know which method of imputation performs better, we will need to perform a cross validation using a decision tree as a baseline since it handles missing values. During the Cross-validation, we will impute the missing values based on the median, or knn of the train set on our validation set. For example, with a 5 fold cross validation, it means that 80 % of our training set will be 4 fold and 20% will represent 1 fold. During the Cross validation, the 4 folds will impute its missing values its median or knn than it will do the same of the other fold (the validation one that represents 20%) based on the training value since we want to avoid data leakage. We will choose the method of imputation that performs the best. Here we will use K-NN, median imputation and dropping values and compare it to not dropping values.

Partitioning our data: We will use the stratified sampling method because we want to ensure that we have the same proportion of water quality in each of our samples. We will create a train set that represents 60% our of data and a validation set that will represents 40 % of our data.

```{r}
set.seed(1)
df$Potability = as.factor(df$Potability) ### changing our Potability variables into a factor since it's a category ! 
train.index = createDataPartition(df$Potability, p = .6, list = FALSE)
train.set = df[ train.index,] ### 60%
val.set= df[-train.index,]
```

```{r}
head(train.set)
```

#### Cross validation with 5 folds for imputation 

Decision tree (without dropping any missing values because DT handle missing values).

```{r}
set.seed(1) 

tr_control = trainControl(method = "cv",number = 5 ,summaryFunction =  multiClassSummary) # Setting 5 fold cross-validation

normal_cv_tree = train(Potability ~ ., #. is to tell that we use all our variables as predictors on personal loan
               data = train.set,    ## on our training data
               method = "rpart",    ### we use classification tree
               na.action = na.pass,
               preProcess = c("center","scale"),
               trControl = tr_control)   ### the cross validation on 10 folds  ### on 30 k
               #,metric = "Kappa"# We can play with metrics to select one specific but here I want them all
               
print(normal_cv_tree)
accu1 = max(normal_cv_tree$results$Accuracy)
f11 = max(normal_cv_tree$results$F1)
#results = data.frame("Accuracy" = accu1, )
```

Decision tree with median imputation.

```{r}
set.seed(1) ### To have the same results  !!! 

tr_control = trainControl(method = "cv",number = 5 ,summaryFunction =  multiClassSummary) ### use cross validation 10 times on 10 different fold and have a summary of all metrics and having a preprocess step of imputing the median to see if it works well

median_cv_tree = train(Potability ~ ., #. is to tell that we use all our variables as predictors on personal loan
               data = train.set,    ## on our training data
               method = "rpart",    ### we use classification tree
               na.action = na.pass,
               trControl = tr_control,
               preProcess = c("medianImpute","center","scale"))   ### the cross validation on 10 folds  ### on 30 k
               #,metric = "Kappa"# We can play with metrics to select one specific but here I want them all
               
print(median_cv_tree)

med_acc = max(median_cv_tree$results$Accuracy)
med_f1 = max(median_cv_tree$results$F1)
```

Decision tree with K-NN imputation. 

```{r}
set.seed(1) 

tr_control = trainControl(method = "cv",number = 5 ,summaryFunction =  multiClassSummary) 

knnimpute_cv_tree = train(Potability ~ ., #. is to tell that we use all our variables as predictors on personal loan
               data = train.set,    ## on our training data
               method = "rpart",    ### we use classification tree
               na.action = na.pass,
               trControl = tr_control,
               preProcess = c("knnImpute","center","scale"))   ### the cross validation on 10 folds  ### on 30 k
               #,metric = "Kappa"# We can play with metrics to select one specific but here I want them all
               
print(knnimpute_cv_tree)

knn_acc = max(knnimpute_cv_tree$results$Accuracy)
knn_f1 = max(knnimpute_cv_tree$results$F1)
```

Decision tree (we drop all the missing values).

```{r}
set.seed(1) ### To have the same results  !!! 

train.set2 = na.omit(train.set)

tr_control = trainControl(method = "cv",number = 5 ,summaryFunction =  multiClassSummary) ### use cross validation 10 times on 10 different fold and have a summary of all metrics and having a preprocess step of imputing the median to see if it works well

drop_cv_tree = train(Potability ~ ., #. is to tell that we use all our variables as predictors on personal loan
               data = train.set2,    ## on our training data
               method = "rpart",    ### we use classification tree
               na.action = na.pass,
               preProcess = c("center","scale"),
               trControl = tr_control)   ### the cross validation on 10 folds  ### on 30 k
               #,metric = "Kappa"# We can play with metrics to select one specific but here I want them all
               
print(drop_cv_tree)

drop_acc = max(drop_cv_tree$results$Accuracy)
drop_f1 = max(drop_cv_tree$results$F1)
```

```{r}
results_cv_impute = data.frame("Accuracy" = c(accu1, med_acc, knn_acc, drop_acc), "F1 Score" = c(f11, med_f1, knn_f1, drop_f1),"Method" = c("No dropping","Median imputation","K-NN imputation","Dropping the NAs"))

results_cv_impute %>% gather(Attributes, Values, 1:2) %>% ggplot(aes(x = Attributes, y = Values, fill = Method)) + scale_fill_viridis_d() + geom_bar(stat = "identity",position = "dodge") + geom_text(mapping= aes(x = Attributes, y=Values,label = round(Values,4)), vjust=-0.5, size = 2,position = position_dodge(width = .9), col = "red") + labs(title = "Comparison of the methods to deal with the NAs", x = "Metrics") + theme_light()
```

It seems that the K-NN imputation is the best method because it has the highest accuracy and the second F1 score. The median imputation has very similar results (but slightly worse). When we do not drop any NAs, we obtain the highest F1 score but it is very close to the K-NN imputation method and median method. However, for the rest of our project, we need to either impute or drop the missing values to perform, for example, a K-NN model or a neural nets. Therefore, the model with the best results and meeting its criteria is the K-NN imputation.

### Preprocessing using K-NN on our train and valid set. WE will do the range and z score method.

we will use 2 different preprocessing methods. The first one using the range method and the second one using the center and scale method. 

```{r}

range_values = preProcess(train.set, method = c("knnImpute", "range")) ### range method that we will use for neural nets and knn mostly

train.range = predict(range_values, train.set)
valid.range = predict(range_values, val.set)


norm_values = preProcess(train.set, method = c("knnImpute", "center","scale")) ### z score method (-mean / sd)
train.norm = predict(norm_values, train.set)
valid.norm = predict(norm_values, val.set)

#test = train.set
#norm_values = preProcess(train.set, method = c("range"))

#test_set = predict(norm_values, test)

```

### Could use this to scale back our variables

```{r}
scale_back_z_score <- function(z,var) {
  sd_train = sd(train.set[,var], na.rm = T)
  mean_train = mean(train.set[,var], na.rm = T)
  x = sapply(z, function(x_) x_*sd_train + mean_train)
  return(x)
}
 
knn_set_scale_back = train.norm

#colnames(train.norm[-10])# Could do this to scale back our data
for (i in colnames(knn_set_scale_back[-10])) {
  knn_set_scale_back[i] = scale_back_z_score(knn_set_scale_back[i],i)
}

```

#### Differences of the distribution when we impute the missing values with and without K-NN:

```{r}
library(reshape2)
ph_group = cbind(train.set$ph, knn_set_scale_back$ph)
ph_group = as.data.frame(ph_group)
colnames(ph_group) = c("ph no imputation", "ph knn imputation")
ph_group = melt(ph_group)
diff_ph = ggplot(ph_group, aes(x=value, group = variable, fill = variable)) + geom_density(alpha = 0.5)+labs(title = "Distribution of PH with and without knn imputation") + scale_fill_manual(values = c("red","blue"))

sulfate_group = cbind(train.set$Sulfate, knn_set_scale_back$Sulfate)
sulfate_group = as.data.frame(sulfate_group)
colnames(sulfate_group) = c("sulfate no imputation", "sulfate knn imputation")
sulfate_group = melt(sulfate_group)
diff_sulfate = ggplot(sulfate_group, aes(x=value, group = variable, fill = variable)) + geom_density(alpha = 0.5)+labs(title = "Distribution of sulfate with and without knn imputation") + scale_fill_manual(values = c("red","blue"))

trihalomethanes_group = cbind(train.set$Trihalomethanes, knn_set_scale_back$Trihalomethanes)
trihalomethanes_group = as.data.frame(trihalomethanes_group)
colnames(trihalomethanes_group) = c("trihalomethanes no imputation", "trihalomethanes knn imputation")
trihalomethanes_group = melt(trihalomethanes_group)
diff_triha = ggplot(trihalomethanes_group, aes(x=value, group = variable, fill = variable)) + geom_density(alpha = 0.5)+labs(title = "Distribution of trihalomethanes with and without knn imputation") + scale_fill_manual(values = c("red","blue"))
```

```{r}
plot_grid(diff_ph, diff_sulfate, diff_triha)
```

The distribution does not really change except for 'Sulfate' variable where we had 25% of missing values. In general, our distribution becomes a bit more centered but the impact does not seem that important.

#### 3D graph 

It represents our water quality based on 'ph', 'Organic_carbon' and 'Turbidity'.
```{r}
fig <- plot_ly(knn_set_scale_back, x = ~ph, y = ~Turbidity, z = ~Organic_carbon, color = ~Potability)
fig <- fig %>% add_markers()
fig <- fig %>% layout(scene = list(xaxis = list(title = 'ph'),
                     yaxis = list(title = 'Turbidity'),
                     zaxis = list(title = 'Organic_carbon')))
fig
```


#### Implemetation of our models : 

##### Let's start with KNN : 
For KNN, it is better to have our train and test set that have value ranged from 0 - 1. So, we will use the train range and valid range sets.
First we need to find the best K for our model so let's do a cross-validation : 
```{r}
set.seed(1) 

tr_control = trainControl(method = "cv",number = 5 ,summaryFunction =  multiClassSummary) 

knn_cross_val = train(Potability ~ ., #. is to tell that we use all our variables as predictors on personal loan
               data = train.range,    ## on our training data
               method = "knn",    ### we use classification tree
               trControl = tr_control, 
               tuneGrid = data.frame(k = seq(50)))
print(knn_cross_val)
knn_cross_val$results[max(knn_cross_val$results$F1) == knn_cross_val$results$F1, c("k","F1")]

```

The best k that maximizes our accuracy and our F1 score at the same time is k = 42

```{r}
library(class)
knn_res = knn(train.range[-10], valid.range[-10], cl=train.range$Potability,k=42, prob = T)
confusion_knn = confusionMatrix(knn_res, valid.range$Potability,positive = "1")
confusion_knn
accuracy_knn = confusion_knn$overall["Accuracy"]
```

as we can see with knn we predicted poorly our class. 61.76 % of accuracy is really not good. We predicted our class 0 (not potable water) better than potable water. Why ? Well it's prbably because we have more non potable water in our data set and having a large k make our prediction based on the more prevalent class which is non potable water.

### Now let's perform a decision tree : 
for a tree we don't need to work with scaled variables so let's scale back a train and test set:
Scaling back our train and validations sets : 
```{r}
train_scaled_back = train.norm
valid_scaled_back = valid.norm
#colnames(train.norm[-10]) Could do this to scale back our data
for (i in colnames(train_scaled_back[-10])) {
  train_scaled_back[i] = scale_back_z_score(train_scaled_back[i],i)
}

for (i in colnames(valid_scaled_back[-10])) {
  valid_scaled_back[i] = scale_back_z_score(valid_scaled_back[i],i)
}
```

Building a full grown tree : 
```{r}
set.seed(1)
train_tree = rpart(Potability ~ . , data=train_scaled_back, method = "class", control=rpart.control(maxdepth=30,cp=0, minsplit = 1,minbucket = 1, xval = 5)) 
```

Pruning our full grown tree using 5 fold cross validation : 
```{r}
library(rattle)
prune_cp.index = which.min(train_tree$cptable[,"xerror"])
prune_cp = train_tree$cptable[prune_cp.index,"CP"]
prune_cp #  0.007822686 CP

pruned_train_tree <- prune(train_tree, cp = prune_cp)
fancyRpartPlot(pruned_train_tree)
```

```{r}
classificationtree_valid_predict = predict(pruned_train_tree, valid_scaled_back , type = "class")
classificationtree_valid_proba = predict(pruned_train_tree, valid_scaled_back , type = "prob")[,2]
confusion_tree = confusionMatrix(classificationtree_valid_predict, valid_scaled_back$Potability, positive = "1")
confusion_tree
accuracy_tree = confusion_tree$overall["Accuracy"]
```

We can see that with a tree we have an accuracy of 63.44% which is better than with knn ! The balanced accuracy is at 56.9 % meaning that we are doing a little bit better than by guessing randomly. It is clear and normal that we get those kind of results since it seems that the data set was weird. Indeed we had same values for all the variables even if they were potable or non potable water

### Naive bayes : don't need scaling
```{r}
library(e1071)
nbayes.model = naiveBayes(Potability ~ ., data = train_scaled_back)
nbayes.predict = predict(nbayes.model, valid_scaled_back)
nbayes_predict_proba = predict(nbayes.model, valid_scaled_back,type = "raw")[,2]
confusion_nb = confusionMatrix(nbayes.predict, valid_scaled_back$Potability, positive = "1")
accuracy_nb = confusion_nb$overall["Accuracy"]
```

Accuracy : 0.6237, Balanced Accuracy : 0.555 but we still struggle to predict well.

### Boosting : 


```{r}
set.seed(1) 
library(fastAdaboost)
library(adabag)
tr_control = trainControl(method = "cv",number = 5 ,summaryFunction =  multiClassSummary) 

boosted_cv = train(Potability ~ ., #. is to tell that we use all our variables as predictors on personal loan
               data = train_scaled_back,    ## on our training data
               method = "adaboost",    ### we use classification tree
               tuneLength=2,
               trControl = tr_control)
print(boosted_cv)
```

Accuracy was used to select the optimal model using the largest value.
The final values used for the model were nIter = 100 and method = Adaboost.M1.

```{r}
set.seed(1)
boost = boosting(Potability~., data = train_scaled_back, type = "class", mfinal = 100)

valid_predict_boost = predict(boost, valid_scaled_back , type = "class")
valid_predict_boost_proba = predict(boost, valid_scaled_back , type = "prob")$prob[,2]
confusion_boost = confusionMatrix(as.factor(valid_predict_boost$class), valid_scaled_back$Potability, positive = "1")
confusion_boost
accuracy_boost = confusion_boost$overall["Accuracy"]
```

```{r}
importanceplot(boost)
```




Accuracy : 0.6359,Balanced Accuracy : 0.5943    Already better. 

### bagging

```{r}
set.seed(1)
tr_control = trainControl(method = "cv",number = 5 ,summaryFunction =  multiClassSummary) 

bagged_cv = train(Potability ~ ., #. is to tell that we use all our variables as predictors on personal loan
               data = train_scaled_back,    ## on our training data
               method = "treebag",    ### we use classification tree
            
               trControl = tr_control
              )
print(bagged_cv)
```

Unfortunately there's no tuning parameter for this method. (But Bagged AdaBoost has ! )

```{r}
set.seed(1)
bag = bagging(Potability~., data = train_scaled_back, type = "class")


valid_predict_bag = predict(bag, valid_scaled_back , type = "class")
valid_predict_bag_prob = valid_predict_bag$prob[,2]
### to get the class, we need to use $class
#valid_predict_boost
confusion_bag = confusionMatrix(as.factor(valid_predict_bag$class), valid_scaled_back$Potability, positive = "1")
confusion_bag
accuracy_bag = confusion_bag$overall["Accuracy"]
```

Accuracy : 0.6511 Balanced Accuracy : 0.56906
```{r}
importanceplot(bag)
```


#### Random Forest
CV

```{r}
library(randomForest)
set.seed(1)
control = trainControl(method='cv', 
                        number=5,
                        search='grid')

tunegrid = expand.grid(.mtry = (1:9)) 

rf_gridsearch = train(Potability ~ ., 
                       data = train_scaled_back,
                       method = 'rf',
                       metric = 'Accuracy',
                       tuneGrid = tunegrid,
                       trControl = control,
                       importance = T)
print(rf_gridsearch)

```

When using cross validation on random forest using the scaled back data, the best mtry to use is 6

```{r}
library(randomForest)
set.seed(1)
randomf = randomForest(Potability~., data = train_scaled_back,mtry = 6, importance = T )


valid_predict_rf = predict(randomf, valid_scaled_back , type = "class")
valid_predict_rf_proba = predict(randomf, valid_scaled_back , type = "prob")[,2]

confusion_rf = confusionMatrix(as.factor(valid_predict_rf), valid_scaled_back$Potability, positive = "1")
confusion_rf
accuracy_rf = confusion_rf$overall["Accuracy"]
varImpPlot(randomf)
```

Accuracy : 0.674
Balanced Accuracy : 0.6228        


#### Logistic Regression

```{r}
logit.reg = glm(Potability ~ ., data = train_scaled_back, family = "binomial")
options(scipen=999)
summary(logit.reg)
```

We can see that no variables are significant ! let's look at which variable we should choose to perform our logistic regression.

```{r}
logi.step = step(logit.reg, direction = "both")
summary(logi.step)
```

Doing a stepwise method we see that we should only keep solids
```{r}
logi_train = train_scaled_back[, c(3,10)]
logi_valid = valid_scaled_back[, c(3,10)]
logistic = glm(Potability~. , data = logi_train, family = "binomial")
summary(logistic)

```

```{r}
best_cutoff = data.frame("cut"=seq(1:99), score = rep(0, 99),f1 = rep(0, 99))
for (i in seq(1:99)) {
  logi_test = predict(logistic, logi_valid, type = "response")
  pred_test= ifelse(logi_test>=i/100,1,0)
  best_cutoff[i,"score"]= confusionMatrix(as.factor(pred_test), logi_valid$Potability,positive="1")$overall[1]
  best_cutoff[i,"f1"]= confusionMatrix(as.factor(pred_test), logi_valid$Potability,positive="1")$byClass["F1"]
  
  
}
best_cutoff[best_cutoff$score == max(best_cutoff$score),]
```

The best accuracy possible is 0.6099

#### Now the same with all our variables to compare
```{r}
best_cutoff = data.frame("cut"=seq(1:99), score = rep(0, 99),f1 = rep(0, 99))
for (i in seq(1:99)) {
  logi_test = predict(logit.reg, valid_scaled_back[,-10], type = "response")
  pred_test= ifelse(logi_test>=i/100,1,0)
  best_cutoff[i,"score"]= confusionMatrix(as.factor(pred_test), valid_scaled_back$Potability,positive="1")$overall[1]
  best_cutoff[i,"f1"]= confusionMatrix(as.factor(pred_test), valid_scaled_back$Potability,positive="1")$byClass["F1"]
  
  
}
best_cutoff[best_cutoff$score == max(best_cutoff$score),]
```

Best cutoff is 46 % to maximize the accuracy : 0.6137405. We can see that this results is close to our baseline : everything is classified as Not potable since it's the predominant class meaning we would get an accuracy of ~ 0.61. So this model really doesn't perform well.

```{r}
pred.logit.reg = predict(logit.reg, valid_scaled_back[,-10], type = "response")

logi_pred_class = ifelse(pred.logit.reg >= 0.46, 1, 0)

confusion_logi = confusionMatrix(as.factor(logi_pred_class), valid_scaled_back$Potability, positive = "1") #Option I think.
confusion_logi

accuracy_logi = confusion_logi$overall["Accuracy"]

#results.df = data.frame(actual = valid.range$Potability, logit.reg = as.factor(ifelse(pred.logit.reg >= 0.46, 1, 0)), logit.reg.prob = pred.logit.reg)

#accuracy.df = data.frame(logit.reg=confusionMatrix(as.factor(ifelse(pred.logit.reg >= 0.46, 1, 0)), valid.range$Potability)$overall[1])

```


We dot better keeping all our variables than just keeping Solids

Let's visualize our results :

```{r}
#library(devtools)
#devtools::install_github('cttobin/ggthemr')

ConfusionMatrixInfo <- function( data, predict, actual, cutoff )
{   
    # extract the column ;
    # relevel making 1 appears on the more commonly seen position in 
    # a two by two confusion matrix 
    predict <- data[[predict]]
    actual  <- relevel( as.factor( data[[actual]] ), "1" )
    
    result <- data.table( actual = actual, predict = predict )

    # caculating each pred falls into which category for the confusion matrix
    result[ , type := ifelse( predict >= cutoff & actual == 1, "TP",
                      ifelse( predict >= cutoff & actual == 0, "FP", 
                      ifelse( predict <  cutoff & actual == 1, "FN", "TN" ) ) ) %>% as.factor() ]

    # jittering : can spread the points along the x axis 
    plot <- ggplot( result, aes( actual, predict, color = type ) ) + 
            geom_violin( fill = "white", color = NA ) +
            geom_jitter( shape = 1 ) + 
            geom_hline( yintercept = cutoff, color = "blue", alpha = 0.6 ) + 
            scale_y_continuous( limits = c( 0, 1 ) ) + 
            scale_color_discrete( breaks = c( "TP", "FN", "FP", "TN" ) ) + # ordering of the legend 
            guides( col = guide_legend( nrow = 2 ) ) + # adjust the legend to have two rows  
            ggtitle( sprintf( "Confusion Matrix with Cutoff at %.2f", cutoff ) )

    return( list( data = result, plot = plot ) )
}
library(data.table)
library(ggthemr)
logi_data = data.frame("prediction" = predict(logit.reg, valid_scaled_back[,-10], type = "response"), "True" = valid_scaled_back$Potability )
cm_info <- ConfusionMatrixInfo( data = logi_data, predict = "prediction", 
                                actual = "True", cutoff = .46 )
ggthemr("flat")
cm_info$plot
```

We can clearly see that it's not very good but it's the best we can do with the logistic regression. We are very close to the baseline of classifying everything into the predominant class (not potable) to get an accuracy of 0.61.

#### Neural nets : Here we need to use the train set and validation set that has been preprocessed using the range method. Also, since our variable were symmetrical we don't need to use the log transformation.

```{r}
set.seed(1)
control <- trainControl(method='cv', 
                        number=5)

neural.net_cv <- train(Potability ~ ., data = train.range,
    method = "nnet",  trControl = control)

neural.net_cv$bestTune
```

Using Cross-validation, we should use 5 nodes in 1 hidden layer and decay = 0.1 is the regularization parameter to avoid over-fitting


```{r}
nn_predict = predict(neural.net_cv, valid.range)
nn_predict_proba = predict(neural.net_cv, valid.range, type = "prob")[,2]
confusion_nn_cv =confusionMatrix(nn_predict, valid.range$Potability, positive = "1")
confusion_nn_cv
accuracy_nn = confusionMatrix(nn_predict, valid.range$Potability, positive = "1")$overall["Accuracy"]
#train_predict_nn = predict(neural.net_cv, train.range) Check if we were overfitting but not at all ! 
#confusionMatrix(train_predict_nn, train.range$Potability, positive = "1")
```

Accuracy : 0.687 which is the best we have seen so far ! 




```{r}
accuracy_all = data.frame("Model" = 
                            c("K-NN","Naive-Bayes","logistic","Decision_tree","Bagging","Boosting","Random_forest","Neural_nets"),
                          "Accuracy"=c(accuracy_knn, accuracy_nb, accuracy_logi, accuracy_tree, accuracy_bag, accuracy_boost, accuracy_rf, accuracy_nn))
ggplot(accuracy_all, aes(x=Model, y = Accuracy, fill = Accuracy)) + geom_bar(stat="identity")
```

```{r}
proba_weird = attr(knn_res,"prob") # Returns only the highest proba so I will compare the proba to the outcome to find if it's the proba of being 1 or 0 ...
knn_res
proba_new_knn = proba_weird + as.numeric(as.character(knn_res))
#proba_new_knn

proba_knn = ifelse(proba_new_knn > 1, proba_new_knn-1, 1-proba_new_knn)


results_all = data.frame("KNN_pred" = knn_res, "KNN_prob" = proba_knn, "logi_pred"=logi_pred_class, "logi_prob" = pred.logit.reg, "NB_pred" = nbayes.predict, "NB_prob" = nbayes_predict_proba, "CT_pred" = classificationtree_valid_predict,"CT_prob" = classificationtree_valid_proba, "bag_pred" = valid_predict_bag$class, "bag_prob" = valid_predict_bag_prob, "boost_pred" =valid_predict_boost$class, "boost_prob" = valid_predict_boost_proba, "rf_pred" = valid_predict_rf, "rf_prob" = valid_predict_rf_proba, "neural_net_pred"=nn_predict, "neural_net_prob" = nn_predict_proba)
head(results_all)
```

Ensemble method : 
```{r}
### Average prediction : WE have 8 models : knn, logi, naive bayes, CT, bag, boost, rf, neural net
ensemble_pred = as.numeric(as.character(results_all$KNN_pred)) + as.numeric(as.character(results_all$logi_pred)) + as.numeric(as.character(results_all$NB_pred)) + as.numeric(as.character(results_all$CT_pred)) +
  as.numeric(as.character(results_all$bag_pred)) + as.numeric(as.character(results_all$boost_pred)) + as.numeric(as.character(results_all$rf_pred)) + as.numeric(as.character(results_all$neural_net_pred))

ensemble_majority = ifelse(ensemble_pred>= 4,1,0)
results_all$ensemble_majority = ensemble_majority


### Average proba : 
results_all$ensemble_proba = ifelse((results_all$KNN_prob +  results_all$logi_prob +  results_all$NB_prob +  results_all$CT_prob +  results_all$bag_prob +  results_all$boost_prob +  results_all$rf_prob +  results_all$neural_net_prob)/8 >= 0.5, 1,0)

results_all$true = valid_scaled_back$Potability
```


```{r}
confusionMatrix(as.factor(results_all$ensemble_majority), valid_scaled_back$Potability,positive = "1")
confusionMatrix(as.factor(results_all$ensemble_proba), valid_scaled_back$Potability,positive = "1")
```

Ensemble average seems to perform better 0.6603 Accuracy. As we can see we have a hard time predicting the Potable water. It is probably because they both have the same distribution of each feature and that we have more non potable water in our data set. Also having some non potable water that has the same distribution as potable water make it hard for any of our model to find which class it belongs to. 




Boosting - Romain

```{r}
set.seed(1)
library(adabag)

boost <- boosting(Potability ~ ., data = train.range)
pred.boost <- predict(boost, valid.range)
boost.cm = confusionMatrix(as.factor(pred.boost$class), valid.range$Potability)
accuracy.df = cbind(accuracy.df, boost.tree=boost.cm$overall[1])

boost.cm
```

As we can see, the boosting method does not predict well.

Bagging - Romain

```{r}
set.seed(1)

bagt <- bagging(Potability ~ ., data = train.range)
pred.bag <- predict(bagt, valid.range)
bag.cm = confusionMatrix(as.factor(pred.bag$class), valid.range$Potability)
accuracy.df = cbind(accuracy.df, bag.tree=bag.cm$overall[1])

bag.cm
```

As we can see, the bagging method does not predict well either.

Random forest - Romain

```{r}
set.seed(1)
library(randomForest)
rf <- randomForest(Potability ~ ., data = train.range, mtry=4, importance = T)
pred.rf <- predict(rf, valid.range)

rf.cm = confusionMatrix(as.factor(pred.rf), valid.range$Potability)
accuracy.df = cbind(accuracy.df, rf=rf.cm$overall[1])

rf.cm
```

Other parameters:
```{r}
set.seed(1)
rf2=randomForest(Potability ~ ., data = train_scaled_back, ntree = 1000, mtry = 5, nodesize = 1, importance = TRUE)
varImpPlot(rf2, type = 1) #Optional I think.
pred.rf2=predict(rf2, valid_scaled_back, type = "class")
rf.cm2=confusionMatrix(pred.rf2, valid_scaled_back$Potability) 
rf.cm2

#We obtain better results with the first parameters
```
As we can see, the random forest method does not predict well either.


Adding K-NN to the Dataframe.
```{r}
accuracy.df = cbind(accuracy.df, knn = confusionMatrix(knn_res, valid.range$Potability)$overall[1])

accuracy.df #Comparing the different methods.
```










