---
title: "Predicting Water Potability"
author: "Romain Kursner & Adrien Kühne"
date: "11/18/2021"
output: html_document
---

### Loading the data set : 
```{r}
rm(list = ls())
setwd("~/Desktop/GSEM Master Business Analytics/1ère Année/Semestre Automne 2021 GSEM/Creating Value Through Data Mining/2021 Lecture/GitHub/cvtdm_project/Project")
df = read.csv("water_potability.csv")
```

### Loading our libraries :
```{r}
library(naniar) #### missing values graph
library(ggplot2) ### plotting library
library(tidyverse)
library(plotly) #### nice interactive graph
library(rpart)
library(rpart.plot)
library(forecast) ### forcast accuracy
library(caret) ### confusion matrix, CV, preprocess and more
library(cowplot) ## multiple ggplot subplot
library(hrbrthemes) ### nice ggplot themes
library(mice) ### missing valiues 
library(pheatmap) ### Nice heatmaps
library(fmsb) ### Radar chart
```

### Exploratory data analysis :

```{r}
str(df)
```

As you can see, our data set has 3276 observations and 10 variables. They are all numerical values except 'Potability' which is an integer. It is normal since 'Potability' indicates if our water is either potable or not. We will need to transform it into a factor later on. We can already see some missing values.

```{r}
summary(df)
```

Here we can see the minimum, maximum, mean and median of all our variables. What is really important to notice is that we have 491 missing values in 'ph', 781 in 'Sulfate' and 162 in 'Trihalomethanes.' We will need to find a good strategy to deal with them.

We could check if we have a balanced data since we are going to classify our water based on the 'Potability' variable.

```{r}
potable_amount = sum(df$Potability == 1)/nrow(df)
notpotable_amount = sum(df$Potability == 0) / nrow(df)
pot_perc = paste0(round(potable_amount,4)*100, "% : Potable")
notpot_perc = paste0(round(notpotable_amount,4)*100, "% : Non-Potable")

amount = data.frame("Group" =c("Potable","Non-Potable"),"Percentage"=c(potable_amount, notpotable_amount), "Labels" = c(pot_perc,notpot_perc))

pie_chart = ggplot(amount, aes(x = "", y = Percentage, fill = Group)) +
  geom_col(color = "black") +
  geom_label(aes(label = Labels), color = c(1, "white"),
            position = position_stack(vjust = 0.5),
            show.legend = FALSE) +
  guides(fill = guide_legend(title = "Quality of the Water"))+
  scale_fill_viridis_d() +
  coord_polar(theta = "y") + labs(title = "Proportion of Potable and Non-Potable Water") +
  theme_void()

amount$total = c(sum(df$Potability == 1),sum(df$Potability == 0))

bar_chart = ggplot(amount, aes(x = Group, y = total, fill = Group)) + geom_bar(stat = "identity") +
  scale_fill_viridis_d() + labs(title = "Quantity of good and bad water in our data set", x = "Water Quality", y = "Total amount", fill = "Water Quality")

plot_grid(pie_chart, bar_chart, labels = "AUTO")

```

We can see that we have more non-potable than potable water. In general, even if we have more non-potable water, our data set is pretty balanced. We have 1278 Potable and 1998 non-potable water.

### Distribution of all our variables by using some boxplots : 

```{r, fig.height=9, fig.width=9}
par(mfrow=c(3,3))
for (i in seq(1:9)) {
  avg_0 = mean(df[df$Potability == 0, i],na.rm = TRUE)
  avg_1 = mean(df[df$Potability == 1, i],na.rm = TRUE)
  means = c(avg_0, avg_1)
  boxplot(df[,i]~df$Potability, main = paste(colnames(df[i])), xlab = "Water Quality", ylab = colnames(df[i]), col = c("#69b3a2", "#404080"))
  points(1:2,means, col = "red", pch = 19)
  legend("topright",legend="Means", col = "red", pch =19)
  
}
```

The distribution of both groups of water seems pretty similar. It is weird. For example, if we look at the 'ph', we can see that some potable water have a 'ph' of 12 and some of 4. It does not seem realistic. 

### Distribution of all our variables by using some histograms : 

```{r}
ggplot(df, aes(x=ph, fill=factor(Potability))) +
    geom_histogram( color="#e9ecef", alpha=0.6, position = 'identity') +
    scale_fill_manual(values=c("#69b3a2", "#404080"), labels = c("Non-potable","Potable")) +
    geom_vline(aes(xintercept = median(df$ph, na.rm=T), color="median")) +
    geom_vline(aes(xintercept = mean(df$ph,na.rm = T), color ="mean")) +
    geom_vline(aes(xintercept =6.5, color="WHO recommendation")) +
    geom_vline(aes(xintercept =8, color="WHO recommendation")) +
    scale_color_manual(values = c("red","blue", "black"))  +
    theme_ipsum() +
    labs(fill="Quality of the water", title= "Distribution of ph for potable and non-potable water", color = "Mean and median")
```

The 'ph' variable seems pretty normally distributed. We can see that the 'ph' variable is close to symmetric (median=mean). The number of variables 'ph' of the non-potable water is higher than the potable one. The WHO recommends to get a pH from 6.5 to 8.

```{r}
ggplot(df, aes(x=Hardness, fill=factor(Potability))) +
    geom_histogram( color="#e9ecef", alpha=0.6, position = 'identity') +
    scale_fill_manual(values=c("#69b3a2", "#404080"), labels = c("Non-potable","Potable")) +
    geom_vline(aes(xintercept = median(df$Hardness, na.rm=T), color="median")) +
    geom_vline(aes(xintercept = mean(df$Hardness,na.rm = T), color ="mean")) +
    scale_color_manual(values = c("red","blue"))  +
    theme_ipsum() +
    labs(fill="Quality of the water", title= "Distribution of Hardness for potable and non-potable water", color = "Mean and median")
#median(df$Hardness, na.rm=T)
#mean(df$Hardness,na.rm = T)
```

The 'Hardness' variable seems pretty normally distributed. We can see that the 'Hardness' variable is close to symmetric (median=mean). The number of variables 'Hardness' of the non-potable water is much higher than the potable one. It seems more centered as well. The WHO does not recommend a limit of 'Hardness'.

```{r}
ggplot(df, aes(x=Solids, fill=factor(Potability))) +
    geom_histogram( color="#e9ecef", alpha=0.6, position = 'identity') +
    scale_fill_manual(values=c("#69b3a2", "#404080"), labels = c("Non-potable","Potable")) +
    geom_vline(aes(xintercept = median(df$Solids, na.rm=T), color="median")) +
    geom_vline(aes(xintercept = mean(df$Solids,na.rm = T), color ="mean")) +
    geom_vline(aes(xintercept = 500, color ="WHO recommendation")) +
    geom_vline(aes(xintercept = 1000, color ="WHO recommendation")) +
    scale_color_manual(values = c("red","blue", "black"))  +
    theme_ipsum() +
    labs(fill="Quality of the water", title= "Distribution of Solids for potable and non-potable water", color = "Mean and median")

```

The 'Solids' variable seems pretty normally distributed. We can see that the 'Solids' variable is not close to symmetric and is right-skewed (median<mean). The number of variables 'Solids' of the non-potable water is much higher than the potable one. The desirable limit of 'Solids' is 500 mg/liter and maximum limit of 'Solids' is 1000mg/liter (WHO). Once again, it seems that our values are not realistic.

```{r}
ggplot(df, aes(x=Chloramines, fill=factor(Potability))) +
    geom_histogram( color="#e9ecef", alpha=0.6, position = 'identity') +
    scale_fill_manual(values=c("#69b3a2", "#404080"), labels = c("Non-potable","Potable")) +
    geom_vline(aes(xintercept = median(df$Chloramines, na.rm=T), color="median")) +
    geom_vline(aes(xintercept = mean(df$Chloramines,na.rm = T), color ="mean")) +
    geom_vline(aes(xintercept = 0, color ="WHO recommendation")) +
    geom_vline(aes(xintercept = 4, color ="WHO recommendation")) +
    scale_color_manual(values = c("red","blue","black"))  +
    theme_ipsum() +
    labs(fill="Quality of the water", title= "Distribution of Chloramines for potable and non-potable water", color = "Mean and median")

```

The 'Chloramines' variable seems pretty normally distributed. We can see that the 'Chloramines' variable is close to symmetric (median=mean). The number of variables 'Chloramines' of the non-potable water is much higher than the potable one. It seems more centered as well. The desirable limit of 'Chloramines' is 4 mg/liter (WHO). Once again, it seems that our values are not realistic.

```{r}
ggplot(df, aes(x=Sulfate, fill=factor(Potability))) +
    geom_histogram( color="#e9ecef", alpha=0.6, position = 'identity') +
    scale_fill_manual(values=c("#69b3a2", "#404080"), labels = c("Non-potable","Potable")) +
    geom_vline(aes(xintercept = median(df$Sulfate, na.rm=T), color="median")) +
    geom_vline(aes(xintercept = mean(df$Sulfate,na.rm = T), color ="mean")) +
    scale_color_manual(values = c("red","blue"))  +
    theme_ipsum() +
    labs(fill="Quality of the water", title= "Distribution of Sulfate for potable and non-potable water", color = "Mean and median")

```

The 'Sulfate' variable seems pretty normally distributed. We can see that the 'Sulfate' variable is close to symmetric (median=mean). The number of variables 'Sulfate' of the non-potable water is much higher than the potable one. It seems more centered as well. The WHO does not recommend a limit of 'Sulfate'.

```{r}
ggplot(df, aes(x=Conductivity, fill=factor(Potability))) +
    geom_histogram( color="#e9ecef", alpha=0.6, position = 'identity') +
    scale_fill_manual(values=c("#69b3a2", "#404080"), labels = c("Non-potable","Potable")) +
    geom_vline(aes(xintercept = median(df$Conductivity, na.rm=T), color="median")) +
    geom_vline(aes(xintercept = mean(df$Conductivity,na.rm = T), color ="mean")) +
    geom_vline(aes(xintercept = 0, color ="WHO recommendation")) +
    geom_vline(aes(xintercept = 400, color ="WHO recommendation")) +
    scale_color_manual(values = c("red","blue", "black"))  +
    theme_ipsum() +
    labs(fill="Quality of the water", title= "Distribution of Conductivity for potable and non-potable water", color = "Mean and median")

```

The 'Conductivity' variable seems pretty normally distributed. Even though the 'Conductivity' variable is slightly right-skewed (median<mean), we can see that it is close to symmetric (median=mean). The number of variables 'Conductivity' of the non-potable water is higher than the potable one. The desirable limit of 'Conductivity' should not exceed 400 μS/cm (WHO). 

```{r}
ggplot(df, aes(x=Organic_carbon, fill=factor(Potability))) +
    geom_histogram( color="#e9ecef", alpha=0.6, position = 'identity') +
    scale_fill_manual(values=c("#69b3a2", "#404080"), labels = c("Non-potable","Potable")) +
    geom_vline(aes(xintercept = median(df$Organic_carbon, na.rm=T), color="median")) +
    geom_vline(aes(xintercept = mean(df$Organic_carbon,na.rm = T), color ="mean")) +
    scale_color_manual(values = c("red","blue"))  +
    theme_ipsum() +
    labs(fill="Quality of the water", title= "Distribution of Organic carbon for potable and non-potable water", color = "Mean and median")

```

The 'Organic_carbon' variable seems pretty normally distributed. We can see that the 'Organic_carbon' variable is close to symmetric (median=mean). The number of variables 'Organic_carbon' of the non-potable water is higher than the potable one. The WHO does not recommend a limit of 'Organic_carbon'.

```{r}
ggplot(df, aes(x=Trihalomethanes, fill=factor(Potability))) +
    geom_histogram( color="#e9ecef", alpha=0.6, position = 'identity') +
    scale_fill_manual(values=c("#69b3a2", "#404080"), labels = c("Non-potable","Potable")) +
    geom_vline(aes(xintercept = median(df$Trihalomethanes, na.rm=T), color="median")) +
    geom_vline(aes(xintercept = mean(df$Trihalomethanes,na.rm = T), color ="mean")) +
    geom_vline(aes(xintercept = 0, color ="WHO recommendation")) +
    geom_vline(aes(xintercept = 80, color ="WHO recommendation")) +
    scale_color_manual(values = c("red","blue","black"))  +
    theme_ipsum() +
    labs(fill="Quality of the water", title= "Distribution of Trihalomethanes for potable and non-potable water", color = "Mean and median")

```

The 'Trihalomethanes' variable seems pretty normally distributed. We can see that the 'Trihalomethanes' variable is close to symmetric (median=mean). The number of variables 'Trihalomethanes' of the non-potable water is higher than the potable one. The desirable limit of 'Trihalomethanes' is 80 ppm.

```{r}
ggplot(df, aes(x=Turbidity, fill=factor(Potability))) +
    geom_histogram( color="#e9ecef", alpha=0.6, position = 'identity') +
    scale_fill_manual(values=c("#69b3a2", "#404080"), labels = c("Non-potable","Potable")) +
    geom_vline(aes(xintercept = median(df$Turbidity, na.rm=T), color="median")) +
    geom_vline(aes(xintercept = mean(df$Turbidity,na.rm = T), color ="mean")) +
    geom_vline(aes(xintercept = 0, color ="WHO recommendation")) +
    geom_vline(aes(xintercept = 5, color ="WHO recommendation")) +
    scale_color_manual(values = c("red","blue","black"))  +
    theme_ipsum() +
    labs(fill="Quality of the water", title= "Distribution of Turbidity for potable and non-potable water", color = "Mean and median")

```

The 'Turbidity' variable seems pretty normally distributed. We can see that the 'Turbidity' variable is close to symmetric (median=mean). The number of variables 'Turbidity' of the non-potable water is higher than the potable one. The desirable limit of 'Turbidity' is 5 NTU (WHO).

### Difference between our potable and non-potable water by using a radar chart.

```{r, fig.width=8, fig.height=8}

par(mfrow=c(1,2))

df_1 = df[df$Potability == 1,]
df_0 = df[df$Potability == 0,]

#### to to put max min and the value

rad_1 = data.frame(ph = c(max(df_1$ph, na.rm = T),0,mean(df_1$ph, na.rm=T)),
                   Hardness = c(max(df_1$Hardness),0,mean(df_1$Hardness, na.rm=T)),
                   Solids = c(max(df_1$Solids),0,mean(df_1$Solids, na.rm=T)),
                   Chloramines = c(max(df_1$Chloramines),0,mean(df_1$Chloramines, na.rm=T)),
                   Sulfate = c(max(df_1$Sulfate, na.rm = T),0,mean(df_1$Sulfate, na.rm=T)),
                   Conductivity = c(max(df_1$Conductivity),0,mean(df_1$Conductivity, na.rm=T)),
                   Organic_Carbon = c(max(df_1$Organic_carbon),0,mean(df_1$Organic_carbon, na.rm=T)),
                   Trihalomethanes = c(max(df_1$Trihalomethanes,na.rm=T),0,mean(df_1$Trihalomethanes, na.rm=T)),
                   Turbidity = c(max(df_1$Turbidity),0,mean(df_1$Turbidity, na.rm=T))
  
                   )

rad_2 = data.frame(ph = c(max(df_0$ph, na.rm = T),0,mean(df_0$ph, na.rm=T)),
                   Hardness = c(max(df_0$Hardness),0,mean(df_0$Hardness, na.rm=T)),
                   Solids = c(max(df_0$Solids),0,mean(df_0$Solids, na.rm=T)),
                   Chloramines = c(max(df_0$Chloramines),0,mean(df_0$Chloramines, na.rm=T)),
                   Sulfate = c(max(df_0$Sulfate, na.rm = T),0,mean(df_0$Sulfate, na.rm=T)),
                   Conductivity = c(max(df_0$Conductivity),0,mean(df_0$Conductivity, na.rm=T)),
                   Organic_Carbon = c(max(df_0$Organic_carbon),0,mean(df_0$Organic_carbon, na.rm=T)),
                   Trihalomethanes = c(max(df_0$Trihalomethanes,na.rm=T),0,mean(df_0$Trihalomethanes, na.rm=T)),
                   Turbidity = c(max(df_0$Turbidity),0,mean(df_0$Turbidity, na.rm=T))

                   )

# Color vector
colors_border=c( rgb(0.2,0.5,0.5,0.9), rgb(0.8,0.2,0.5,0.9) , rgb(0.7,0.5,0.1,0.9) )
colors_in=c( rgb(0.2,0.5,0.5,0.4), rgb(0.8,0.2,0.5,0.4) , rgb(0.7,0.5,0.1,0.4) )

radarchart(rad_1  , axistype=1 ,
 #custom polygon
    pcol=colors_border , pfcol=colors_in , plwd=4 , plty=1,
    #custom the grid
    cglcol="grey", cglty=1, axislabcol="grey", cglwd=0.8,
    #custom labels
    vlcex=0.8 , title = "Potable Water"
    
    )

radarchart(rad_2  , axistype=1 ,
 #custom polygon
    pcol=colors_border , pfcol=colors_in , plwd=4 , plty=1,
    #custom the grid
    cglcol="grey", cglty=1, axislabcol="grey", cglwd=0.8,
    #custom labels
    vlcex=0.8 , title = "Not Potable Water"
    
    )
```

We used the maximum values and 0 when potability = 1 and potability = 0 as the range of the radar chart. We used the mean of each feature for potability = 1 and potability = 0 to represent their characteristics. We can see that there's not a significant difference between them.

```{r}
new_rad = data.frame(ph = c(max(df$ph, na.rm = T),0,mean(df_0$ph, na.rm=T),mean(df_1$ph, na.rm=T)),
                   Hardness = c(max(df_0$Hardness),0,mean(df_0$Hardness, na.rm=T),mean(df_1$Hardness, na.rm=T)),
                   Solids = c(max(df_0$Solids),0,mean(df_0$Solids, na.rm=T),mean(df_1$Solids, na.rm=T)),
                   Chloramines = c(max(df_0$Chloramines),0,mean(df_0$Chloramines, na.rm=T),mean(df_1$Chloramines, na.rm=T)),
                   Sulfate = c(max(df_0$Sulfate, na.rm = T),0,mean(df_0$Sulfate, na.rm=T),mean(df_1$Sulfate, na.rm=T)),
                   Conductivity = c(max(df_0$Conductivity),0,mean(df_0$Conductivity, na.rm=T),mean(df_1$Conductivity, na.rm=T)),
                   Organic_Carbon = c(max(df_0$Organic_carbon),0,mean(df_0$Organic_carbon, na.rm=T),mean(df_1$Organic_carbon, na.rm=T)),
                   Trihalomethanes = c(max(df_0$Trihalomethanes,na.rm=T),0,mean(df_0$Trihalomethanes, na.rm=T),mean(df_1$Trihalomethanes,na.rm=T)),
                   Turbidity = c(max(df_0$Turbidity),0,mean(df_0$Turbidity, na.rm=T),mean(df_1$Turbidity, na.rm=T))

                   )

row.names(new_rad)[3:4] = c("Not Potable", "Potable")


# Color vector
colors_border=c( rgb(0.2,0.5,0.5,0.9), rgb(0.8,0.2,0.5,0.9) )
colors_in=c( rgb(0.2,0.5,0.5,0.4), rgb(0.8,0.2,0.5,0.4))

# plot with default options:
radarchart( new_rad  , axistype=1 , 
    #custom polygon
    pcol=colors_border , pfcol=colors_in , plwd=4 , plty=1,
    #custom the grid
    cglcol="grey", cglty=1, axislabcol="grey", cglwd=0.8,
    #custom labels
    vlcex=0.8 
    )

# Add a legend
legend(x=0.7, y=1, legend = rownames(new_rad[-c(1,2),]), bty = "n", pch=20 , col=colors_in , text.col = "grey", cex=1.2, pt.cex=3)
new_rad

```

In fact, we can see that there is no significant difference between potable and non-potable water. It does not seem realistic at all ! We would expect at least differences between some features.

### Correlation of our variables : 

```{r}
pheatmap(cor(df, use = "complete"), display_numbers = T,cluster_rows = F, cluster_cols = F, fontsize_number = 15)
```

They are not correlated at all between each other. It means that we will not have issues of multicolinearity.

#### Missing values :

```{r}
gg_miss_var(df, show_pct = TRUE)
```

Our initial dataset has 3276 rows and 10 features. It has 1278 observations of potable water and 1998 of non-potable water.
If we drop the missing values, our dataset has 2011 rows and 10 features. It has 811 observations of potable water and 1200 of non-potable water. We would lose 1265 observations. It seems that we have more missing values for the non-potable water.
We can see that almost 25% of the 'Sulfate' and 15% of 'ph' observations have missing values. This is a huge proportion. In comparison, we note that only 5% of the 'Trihalomethanes' observations have missing value. We could check whether the missing values follow a pattern.

#### Potential pattern of the missing values :

```{r}
library(missRanger)
library(dlookr)
plot_na_intersect(df)
```

```{r}
plot_na_pareto(df)
```

```{r}
md.pattern(df,rotate.names = TRUE, plot=TRUE)
```

```{r}
library(visdat)
vis_miss(df)
```

Looking at all the previous graphs, it seems that the missing observations do not really follow a pattern. It does not seem that the NAs are missing in a systematic way.

#### Comparing different methods to find the best one to deal with the missing values. 

Partitioning our data: We will use the stratified sampling method because we want to ensure that we have the same proportion of water quality in each of our samples. We will create a train set that represents 60% our of data and a validation set that will represents 40 % of our data.

```{r}
set.seed(1)
df$Potability = as.factor(df$Potability) ### changing our Potability variables into a factor since it's a category ! 
train.index = createDataPartition(df$Potability, p = .6, list = FALSE)
train.set = df[ train.index,] ### 60%
val.set= df[-train.index,]
```

```{r}
head(train.set)
```

#### Cross validation with 5 folds for imputation 

Decision tree (without dropping any missing values because DT handle missing values).

```{r}
set.seed(1) 

tr_control = trainControl(method = "cv",number = 5 ,summaryFunction =  multiClassSummary) # Setting 5 fold cross-validation

normal_cv_tree = train(Potability ~ ., #. is to tell that we use all our variables as predictors on personal loan
               data = train.set,    ## on our training data
               method = "rpart",    ### we use classification tree
               na.action = na.pass,
               preProcess = c("center","scale"),
               trControl = tr_control)   ### the cross validation on 10 folds  ### on 30 k
               #,metric = "Kappa"# We can play with metrics to select one specific but here I want them all
               
print(normal_cv_tree)
accu1 = max(normal_cv_tree$results$Accuracy)
f11 = max(normal_cv_tree$results$F1)
#results = data.frame("Accuracy" = accu1, )
```

Decision tree with median imputation.

```{r}
set.seed(1) ### To have the same results  !!! 

tr_control = trainControl(method = "cv",number = 5 ,summaryFunction =  multiClassSummary) ### use cross validation 10 times on 10 different fold and have a summary of all metrics and having a preprocess step of imputing the median to see if it works well

median_cv_tree = train(Potability ~ ., #. is to tell that we use all our variables as predictors on personal loan
               data = train.set,    ## on our training data
               method = "rpart",    ### we use classification tree
               na.action = na.pass,
               trControl = tr_control,
               preProcess = c("medianImpute","center","scale"))   ### the cross validation on 10 folds  ### on 30 k
               #,metric = "Kappa"# We can play with metrics to select one specific but here I want them all
               
print(median_cv_tree)

med_acc = max(median_cv_tree$results$Accuracy)
med_f1 = max(median_cv_tree$results$F1)
```

Decision tree with K-NN imputation. 

```{r}
set.seed(1) 

tr_control = trainControl(method = "cv",number = 5 ,summaryFunction =  multiClassSummary) 

knnimpute_cv_tree = train(Potability ~ ., #. is to tell that we use all our variables as predictors on personal loan
               data = train.set,    ## on our training data
               method = "rpart",    ### we use classification tree
               na.action = na.pass,
               trControl = tr_control,
               preProcess = c("knnImpute","center","scale"))   ### the cross validation on 10 folds  ### on 30 k
               #,metric = "Kappa"# We can play with metrics to select one specific but here I want them all
               
print(knnimpute_cv_tree)

knn_acc = max(knnimpute_cv_tree$results$Accuracy)
knn_f1 = max(knnimpute_cv_tree$results$F1)
```

Decision tree (we drop all the missing values).

```{r}
set.seed(1) ### To have the same results  !!! 

train.set2 = na.omit(train.set)

tr_control = trainControl(method = "cv",number = 5 ,summaryFunction =  multiClassSummary) ### use cross validation 10 times on 10 different fold and have a summary of all metrics and having a preprocess step of imputing the median to see if it works well

drop_cv_tree = train(Potability ~ ., #. is to tell that we use all our variables as predictors on personal loan
               data = train.set2,    ## on our training data
               method = "rpart",    ### we use classification tree
               na.action = na.pass,
               preProcess = c("center","scale"),
               trControl = tr_control)   ### the cross validation on 10 folds  ### on 30 k
               #,metric = "Kappa"# We can play with metrics to select one specific but here I want them all
               
print(drop_cv_tree)

drop_acc = max(drop_cv_tree$results$Accuracy)
drop_f1 = max(drop_cv_tree$results$F1)
```

```{r}
results_cv_impute = data.frame("Accuracy" = c(accu1, med_acc, knn_acc, drop_acc), "F1 Score" = c(f11, med_f1, knn_f1, drop_f1),"Method" = c("No dropping","Median imputation","K-NN imputation","Dropping the NAs"))

results_cv_impute %>% gather(Attributes, Values, 1:2) %>% ggplot(aes(x = Attributes, y = Values, fill = Method)) + scale_fill_viridis_d() + geom_bar(stat = "identity",position = "dodge") + geom_text(mapping= aes(x = Attributes, y=Values,label = round(Values,4)), vjust=-0.5, size = 2,position = position_dodge(width = .9), col = "red") + labs(title = "Comparison of the methods to deal with the NAs", x = "Metrics") + theme_light()
```

It seems that the K-NN imputation is the best method because it has the highest accuracy and the second F1 score. The median imputation has very similar results (but slightly worse). When we do not drop any NAs, we obtain the highest F1 score but it is very close to the K-NN imputation method and median method. However, for the rest of our project, we need to either impute or drop the missing values to perform, for example, a K-NN model or a neural nets. Therefore, the model with the best results and meeting its criteria is the K-NN imputation.

### Preprocessing

we will use 2 different preprocessing methods. The first one using the range method and the second one using the center and scale method. 

```{r}

range_values = preProcess(train.set, method = c("knnImpute", "range")) ### range method that we will use for neural nets and knn mostly

train.range = predict(range_values, train.set)
valid.range = predict(range_values, val.set)


norm_values = preProcess(train.set, method = c("knnImpute", "center","scale")) ### z score method (-mean / sd)
train.norm = predict(norm_values, train.set)
valid.norm = predict(norm_values, val.set)

#test = train.set
#norm_values = preProcess(train.set, method = c("range"))

#test_set = predict(norm_values, test)

```

### Could use this to scale back our variables

```{r}
scale_back_z_score <- function(z,var) {
  sd_train = sd(train.set[,var], na.rm = T)
  mean_train = mean(train.set[,var], na.rm = T)
  x = sapply(z, function(x_) x_*sd_train + mean_train)
  return(x)
}
 
knn_set_scale_back = train.norm

#colnames(train.norm[-10]) Could do this to scale back our data
#for (i in colnames(knn_set_scale_back[-10])) {
 # knn_set_scale_back[i] = scale_back_z_score(knn_set_scale_back[i],i)
#}

```

#### Differences of the distribution when we impute the missing values with and without K-NN:

```{r}
library(reshape2)
ph_group = cbind(train.set$ph, knn_set_scale_back$ph)
ph_group = as.data.frame(ph_group)
colnames(ph_group) = c("ph no imputation", "ph knn imputation")
ph_group = melt(ph_group)
diff_ph = ggplot(ph_group, aes(x=value, group = variable, fill = variable)) + geom_density(alpha = 0.5)+labs(title = "Distribution of PH with and without knn imputation") + scale_fill_manual(values = c("red","blue"))

sulfate_group = cbind(train.set$Sulfate, knn_set_scale_back$Sulfate)
sulfate_group = as.data.frame(sulfate_group)
colnames(sulfate_group) = c("sulfate no imputation", "sulfate knn imputation")
sulfate_group = melt(sulfate_group)
diff_sulfate = ggplot(sulfate_group, aes(x=value, group = variable, fill = variable)) + geom_density(alpha = 0.5)+labs(title = "Distribution of sulfate with and without knn imputation") + scale_fill_manual(values = c("red","blue"))

trihalomethanes_group = cbind(train.set$Trihalomethanes, knn_set_scale_back$Trihalomethanes)
trihalomethanes_group = as.data.frame(trihalomethanes_group)
colnames(trihalomethanes_group) = c("trihalomethanes no imputation", "trihalomethanes knn imputation")
trihalomethanes_group = melt(trihalomethanes_group)
diff_triha = ggplot(trihalomethanes_group, aes(x=value, group = variable, fill = variable)) + geom_density(alpha = 0.5)+labs(title = "Distribution of trihalomethanes with and without knn imputation") + scale_fill_manual(values = c("red","blue"))
```

```{r}
plot_grid(diff_ph, diff_sulfate, diff_triha)
```

The distribution does not really change except for 'Sulfate' variable where we had 25% of missing values. In general, our distribution becomes a bit more centered but the impact does not seem that important.

#### 3D graph 

It represents our water quality based on 'ph', 'Organic_carbon' and 'Turbidity'.
```{r}
fig <- plot_ly(knn_set_scale_back, x = ~ph, y = ~Turbidity, z = ~Organic_carbon, color = ~Potability)
fig <- fig %>% add_markers()
fig <- fig %>% layout(scene = list(xaxis = list(title = 'ph'),
                     yaxis = list(title = 'Turbidity'),
                     zaxis = list(title = 'Organic_carbon')))
fig
```


#### Implemetation of our models : 

##### Let's start with KNN : 
For KNN, it is better to have our train and test set that have value ranged from 0 - 1. So, we will use the train range and valid range sets.
First we need to find the best K for our model so let's do a cross-validation : 
```{r}
set.seed(1) 

tr_control = trainControl(method = "cv",number = 5 ,summaryFunction =  multiClassSummary) 

knn_cross_val = train(Potability ~ ., #. is to tell that we use all our variables as predictors on personal loan
               data = train.range,    ## on our training data
               method = "knn",    ### we use classification tree
               trControl = tr_control, 
               tuneGrid = data.frame(k = seq(50)))
print(knn_cross_val)
knn_cross_val$results[max(knn_cross_val$results$F1) == knn_cross_val$results$F1, c("k","F1")]

```

The best k that maximizes our accuracy and our F1 score at the same time is k = 42

```{r}
library(class)
knn_res = knn(train.range[-10], valid.range[-10], cl=train.range$Potability,k=42)
confusionMatrix(knn_res, valid.range$Potability,positive = "1")
```

as we can see with knn we predicted poorly our class.

### Now let's perform a decision tree : 
for a tree we don't need to work with scaled variables so let's scale back a train and test set:
Scaling back our train and validations sets : 
```{r}
train_scaled_back = train.norm
valid_scaled_back = valid.norm
#colnames(train.norm[-10]) Could do this to scale back our data
for (i in colnames(train_scaled_back[-10])) {
  train_scaled_back[i] = scale_back_z_score(train_scaled_back[i],i)
}

for (i in colnames(valid_scaled_back[-10])) {
  valid_scaled_back[i] = scale_back_z_score(valid_scaled_back[i],i)
}
```















